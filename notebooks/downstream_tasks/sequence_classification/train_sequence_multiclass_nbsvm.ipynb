{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus: wiki_lm_lstm\n",
      "- Already up to date.\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "from datasets import load_dataset\n",
    "from thai2transformers.metrics import classification_metrics\n",
    "from pythainlp.ulmfit import process_thai\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters\n",
    "class Args:\n",
    "    dataset_name_or_path = 'wisesight_sentiment'\n",
    "    feature_col = 'texts'\n",
    "    label_col = 'category'\n",
    "    metric_for_best_model = 'f1_micro'\n",
    "    seed = 1412\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset wisesight_sentiment (/Users/admin/.cache/huggingface/datasets/wisesight_sentiment/wisesight_sentiment/1.0.0/4bb1772cff1a0703d72fb9e84dff9348e80f6cdf80b0f6c0f59bcd85fc5a3537)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['texts', 'category'],\n",
       "        num_rows: 21628\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['texts', 'category'],\n",
       "        num_rows: 2404\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['texts', 'category'],\n",
       "        num_rows: 2671\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(args.dataset_name_or_path)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['texts', 'category'],\n",
       "        num_rows: 21628\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['texts', 'category'],\n",
       "        num_rows: 2404\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['texts', 'category'],\n",
       "        num_rows: 2671\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if args.dataset_name_or_path == 'wongnai_reviews':\n",
    "    train_val_split = dataset['train'].train_test_split(test_size=0.1, shuffle=True, seed=2020)\n",
    "    dataset['train'] = train_val_split['train']\n",
    "    dataset['validation'] = train_val_split['test']\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nbsvm class\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.validation import check_X_y, check_is_fitted\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy import sparse\n",
    "\n",
    "class NbSvmClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, penalty='l2', C=1.0, dual=False, seed=1412):\n",
    "        self.penalty = penalty\n",
    "        self.C = C\n",
    "        self.dual = dual\n",
    "        self.seed = seed\n",
    "        \n",
    "    def predict(self, x):\n",
    "        # Verify that model has been fit\n",
    "        check_is_fitted(self, ['_r', '_clf'])\n",
    "        return self._clf.predict(x.multiply(self._r))\n",
    "\n",
    "    def predict_proba(self, x):\n",
    "        # Verify that model has been fit\n",
    "        check_is_fitted(self, ['_r', '_clf'])\n",
    "        return self._clf.predict_proba(x.multiply(self._r))\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        # Check that X and y have correct shape\n",
    "        y = y.toarray().ravel() if type(y)!=np.ndarray else y.ravel()\n",
    "        x, y = check_X_y(x, y, accept_sparse=True)\n",
    "\n",
    "        def pr(x, y_i, y):\n",
    "            p = x[y==y_i].sum(0)\n",
    "            return (p+1) / ((y==y_i).sum()+1)\n",
    "\n",
    "        self._r = sparse.csr_matrix(np.log(pr(x,1,y) / pr(x,0,y)))\n",
    "        x_nb = x.multiply(self._r)\n",
    "        self._clf = LogisticRegression(penalty = self.penalty, \n",
    "                                       C=self.C, \n",
    "                                       dual=self.dual,\n",
    "                                       solver='liblinear',\n",
    "                                       random_state=self.seed,).fit(x_nb, y)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.dataset_name_or_path == 'generated_reviews_enth':\n",
    "    texts_train = [i['th'] for i in dataset['train'][args.feature_col]]\n",
    "    texts_valid = [i['th'] for i in dataset['validation'][args.feature_col]]\n",
    "    texts_test = [i['th'] for i in dataset['test'][args.feature_col]]\n",
    "else:\n",
    "    texts_train = dataset['train'][args.feature_col]\n",
    "    texts_valid = dataset['validation'][args.feature_col]\n",
    "    texts_test = dataset['test'][args.feature_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<21628x38120 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 684096 stored elements in Compressed Sparse Row format>,\n",
       " <2404x38120 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 72836 stored elements in Compressed Sparse Row format>,\n",
       " <2671x38120 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 80819 stored elements in Compressed Sparse Row format>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#x\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,2), tokenizer=process_thai,\n",
    "               min_df=3, max_df=0.9, strip_accents='unicode', use_idf=1,\n",
    "               smooth_idf=1, sublinear_tf=1 )\n",
    "\n",
    "x_train = tfidf.fit_transform(texts_train)\n",
    "x_valid = tfidf.transform(texts_valid)\n",
    "x_test = tfidf.transform(texts_test)\n",
    "x_train,x_valid,x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<21628x4 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 21628 stored elements in Compressed Sparse Row format>,\n",
       " <2404x4 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 2404 stored elements in Compressed Sparse Row format>,\n",
       " <2671x4 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 2671 stored elements in Compressed Sparse Row format>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#y\n",
    "if args.dataset_name_or_path == 'generated_reviews_enth' and args.label_col=='review_star':\n",
    "    labels_train = [i-1 for i in dataset['train'][args.label_col]]\n",
    "    labels_valid = [i-1 for i in dataset['validation'][args.label_col]]\n",
    "    labels_test = [i-1 for i in dataset['test'][args.label_col]]\n",
    "else:\n",
    "    labels_train = dataset['train'][args.label_col]\n",
    "    labels_valid = dataset['validation'][args.label_col]\n",
    "    labels_test = dataset['test'][args.label_col]\n",
    "    \n",
    "    \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "y_train = enc.fit_transform(np.array(labels_train)[:,None])\n",
    "y_valid = enc.transform(np.array(labels_valid)[:,None])\n",
    "y_test = enc.transform(np.array(labels_test)[:,None])\n",
    "y_train,y_valid,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#validation\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "def validation_f1(penalty, C, seed):\n",
    "    probs = np.zeros((x_valid.shape[0], y_valid.shape[1]))\n",
    "    for i in range(len(enc.categories_[0])):\n",
    "        if penalty == 'l1':\n",
    "            model = NbSvmClassifier(penalty='l1', \n",
    "                                    C=C, \n",
    "                                    dual=False,\n",
    "                                    seed=seed).fit(x_train, y_train[:,i])\n",
    "        else:\n",
    "            model = NbSvmClassifier(penalty='l2', \n",
    "                                    C=C, \n",
    "                                    dual=True,\n",
    "                                    seed=seed).fit(x_train, y_train[:,i])\n",
    "        probs[:,i] = model.predict_proba(x_valid)[:,1]\n",
    "\n",
    "        preds = probs.argmax(1)\n",
    "    return f1_score(labels_valid, preds, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>penalty</th>\n",
       "      <th>C</th>\n",
       "      <th>f1_micro</th>\n",
       "      <th>dual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wisesight_sentiment</td>\n",
       "      <td>l2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.720466</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wisesight_sentiment</td>\n",
       "      <td>l2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.718386</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wisesight_sentiment</td>\n",
       "      <td>l2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.715474</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wisesight_sentiment</td>\n",
       "      <td>l1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.710067</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wisesight_sentiment</td>\n",
       "      <td>l1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.707571</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>wisesight_sentiment</td>\n",
       "      <td>l2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.707571</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>wisesight_sentiment</td>\n",
       "      <td>l1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.706323</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>wisesight_sentiment</td>\n",
       "      <td>l1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.705075</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               dataset penalty  C  f1_micro   dual\n",
       "0  wisesight_sentiment      l2  3  0.720466   True\n",
       "1  wisesight_sentiment      l2  2  0.718386   True\n",
       "2  wisesight_sentiment      l2  4  0.715474   True\n",
       "3  wisesight_sentiment      l1  2  0.710067  False\n",
       "4  wisesight_sentiment      l1  3  0.707571  False\n",
       "5  wisesight_sentiment      l2  1  0.707571   True\n",
       "6  wisesight_sentiment      l1  1  0.706323  False\n",
       "7  wisesight_sentiment      l1  4  0.705075  False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperparams = []\n",
    "for p in ['l1','l2']:\n",
    "    for c in range(1,5):\n",
    "        hyp = {'dataset':args.dataset_name_or_path,\n",
    "               'penalty':p,\n",
    "               'C':c,\n",
    "               'f1_micro':validation_f1(p,c,seed=args.seed)}\n",
    "        hyp['dual'] = True if p=='l2' else False\n",
    "        hyperparams.append(hyp)\n",
    "hyperparams_df = pd.DataFrame(hyperparams).sort_values('f1_micro',ascending=False).reset_index(drop=True)\n",
    "best_hyperparams = hyperparams_df.drop(['f1_micro','dataset'],1).iloc[0,:].to_dict()\n",
    "hyperparams_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_micro</th>\n",
       "      <th>precision_micro</th>\n",
       "      <th>recall_micro</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>precision_macro</th>\n",
       "      <th>recall_macro</th>\n",
       "      <th>nb_samples</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.720329</td>\n",
       "      <td>0.720329</td>\n",
       "      <td>0.720329</td>\n",
       "      <td>0.720329</td>\n",
       "      <td>0.546664</td>\n",
       "      <td>0.661797</td>\n",
       "      <td>0.511304</td>\n",
       "      <td>2671.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy  f1_micro  precision_micro  recall_micro  f1_macro  \\\n",
       "0  0.720329  0.720329         0.720329      0.720329  0.546664   \n",
       "\n",
       "   precision_macro  recall_macro  nb_samples  \n",
       "0         0.661797      0.511304      2671.0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test\n",
    "probs = np.zeros((x_test.shape[0], y_test.shape[1]))\n",
    "for i in range(len(enc.categories_[0])):\n",
    "    model = NbSvmClassifier(**best_hyperparams).fit(x_train, y_train[:,i])\n",
    "    probs[:,i] = model.predict_proba(x_test)[:,1]\n",
    "\n",
    "class Preds:\n",
    "    label_ids = labels_test\n",
    "    predictions = probs\n",
    "    \n",
    "pd.DataFrame.from_dict(classification_metrics(Preds),orient='index').transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({2: 683, 1: 1453, 0: 478, 3: 57})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
