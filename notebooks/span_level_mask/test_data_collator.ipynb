{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../thai2transformers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer , DataCollatorForLanguageModeling\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.6.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('airesearchth/wangchanberta-base-wiki-20210520-spm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import List, Dict, Union, Optional, Tuple, Any\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "from bisect import bisect\n",
    "from transformers.data.data_collator import DataCollatorForLanguageModeling, _collate_batch, tolist\n",
    "from transformers.tokenization_utils_base import BatchEncoding, PreTrainedTokenizerBase\n",
    "\n",
    "SPECIAL_TOKEN_NAMES = ['bos_token', 'eos_token', 'sep_token', 'cls_token', 'pad_token']\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForSpanLevelMask(DataCollatorForLanguageModeling):\n",
    "    \"\"\"\n",
    "    Data collator used for span-level masked language modeling\n",
    "     \n",
    "    adapted from NGramMaskGenerator class\n",
    "    \n",
    "    https://github.com/microsoft/DeBERTa/blob/11fa20141d9700ba2272b38f2d5fce33d981438b/DeBERTa/apps/tasks/mlm_task.py#L36\n",
    "    and\n",
    "    https://github.com/zihangdai/xlnet/blob/0b642d14dd8aec7f1e1ecbf7d6942d5faa6be1f0/data_utils.py\n",
    "\n",
    "    \"\"\"\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    mlm: bool = True\n",
    "    mlm_probability: float = 0.15\n",
    "    max_gram: int = 3\n",
    "    keep_prob: float = 0.0\n",
    "    mask_prob: float = 1.0\n",
    "    max_preds_per_seq: int = None\n",
    "    max_seq_len: int = 510\n",
    "\n",
    "    def __init__(self, tokenizer, mlm=True, mlm_probability=0.15, *args, **kwargs):\n",
    "        super().__init__(tokenizer, mlm=mlm, mlm_probability=mlm_probability)\n",
    "\n",
    "        assert self.mask_prob + self.keep_prob <= 1, \\\n",
    "            f'The prob of using [MASK]({self.mask_prob}) and the prob of using original token({self.keep_prob}) should between [0,1]'\n",
    "\n",
    "        if self.max_preds_per_seq is None:\n",
    "            self.max_preds_per_seq = math.ceil(self.max_seq_len * self.mlm_probability / 10) * 10\n",
    "            self.mask_window = int(1 / self.mlm_probability) # make ngrams per window sized context\n",
    "        self.vocab_words = list(self.tokenizer.get_vocab().keys())\n",
    "        self.vocab_mapping = self.tokenizer.get_vocab()\n",
    "        \n",
    "        self.special_tokens = [self.tokenizer.special_tokens_map[name] for name in  SPECIAL_TOKEN_NAMES]\n",
    "#         print(' self.special_tokens', self.special_tokens)\n",
    "        self.ngrams = np.arange(1, self.max_gram + 1, dtype=np.int64)\n",
    "        _pvals = 1. / np.arange(1, self.max_gram + 1)\n",
    "        self.pvals = _pvals / _pvals.sum(keepdims=True)\n",
    "\n",
    "    def _choice(self, rng, data, p):\n",
    "        cul = np.cumsum(p)\n",
    "        x = rng.random()*cul[-1]\n",
    "        id = bisect(cul, x)\n",
    "        return data[id]\n",
    "\n",
    "    def _per_token_mask(self, idx, tokens, rng, mask_prob, keep_prob):\n",
    "        label = tokens[idx]\n",
    "        mask = self.tokenizer.mask_token\n",
    "        rand = rng.random()\n",
    "        if rand < mask_prob:\n",
    "            new_label = mask\n",
    "        elif rand < mask_prob + keep_prob:\n",
    "            new_label = label\n",
    "        else:\n",
    "            new_label = rng.choice(self.vocab_words)\n",
    "\n",
    "        tokens[idx] = new_label\n",
    "\n",
    "        return label\n",
    "\n",
    "    def _mask_tokens(self, tokens: List[str], rng=random, **kwargs):\n",
    "\n",
    "        indices = [i for i in range(len(tokens)) if tokens[i] not in self.special_tokens]\n",
    "#         print('debug: indices to be able to be masked', indices)\n",
    "        \n",
    "        unigrams = [ [idx] for idx in indices ]\n",
    "        num_to_predict = min(self.max_preds_per_seq, max(1, int(round(len(tokens) * self.mlm_probability))))\n",
    "           \n",
    "        offset = 0\n",
    "        mask_grams = np.array([False]*len(unigrams))\n",
    "        while offset < len(unigrams):\n",
    "            n = self._choice(rng, self.ngrams, p=self.pvals)\n",
    "            ctx_size = min(n * self.mask_window, len(unigrams)-offset)\n",
    "            m = rng.randint(0, ctx_size-1)\n",
    "            s = offset + m\n",
    "            e = min(offset + m + n, len(unigrams))\n",
    "            offset = max(offset+ctx_size, e)\n",
    "            mask_grams[s:e] = True\n",
    "\n",
    "        target_labels = [None]*len(tokens)\n",
    "        w_cnt = 0\n",
    "        for m,word in zip(mask_grams, unigrams):\n",
    "            if m:\n",
    "                for idx in word:\n",
    "                    label = self._per_token_mask(idx, tokens, rng, self.mask_prob, self.keep_prob)\n",
    "                    target_labels[idx] = label\n",
    "                    w_cnt += 1\n",
    "                if w_cnt >= num_to_predict:\n",
    "                    break\n",
    "\n",
    "        target_labels = [self.vocab_mapping[x] if x else -100 for x in target_labels]\n",
    "        return tokens, target_labels    \n",
    "\n",
    "\n",
    "    def mask_tokens(\n",
    "        self, inputs: torch.Tensor, special_tokens_mask: Optional[torch.Tensor] = None\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original.\n",
    "        \"\"\"\n",
    "        labels = []\n",
    "        # We sample a few tokens in each sequence for MLM training (with probability `self.mlm_probabilityability`)\n",
    "        # probability_matrix = torch.full(labels.shape, self.mlm_probabilityability)\n",
    "        # if special_tokens_mask is None:\n",
    "        #     special_tokens_mask = [\n",
    "        #         self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()\n",
    "        #     ]\n",
    "        #     special_tokens_mask = torch.tensor(special_tokens_mask, dtype=torch.bool)\n",
    "        # else:\n",
    "        #     special_tokens_mask = special_tokens_mask.bool()\n",
    "\n",
    "#         print('inputs', inputs.shape, inputs)\n",
    "        inputs_masked = []\n",
    "        \n",
    "        for i, input in enumerate(inputs):\n",
    "#             print('input',input)\n",
    "            input_tokens = self.tokenizer.convert_ids_to_tokens(input)\n",
    "            \n",
    "\n",
    "            input_masked, _labels = self._mask_tokens(input_tokens)\n",
    "#             print('DEBUG: input_masked', input_masked)\n",
    "            input_masked_ids = self.tokenizer.convert_tokens_to_ids(input_masked)\n",
    "            inputs_masked.append(input_masked_ids)\n",
    "#             print('_labels, ', _labels)\n",
    "#             print('inputs_masked, ', input_masked_ids)\n",
    "            labels.append(_labels)\n",
    "      \n",
    "        return inputs_masked, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5000, 0.5000, 0.5000, 0.5000, 0.5000],\n",
       "        [0.5000, 0.5000, 0.5000, 0.5000, 0.5000],\n",
       "        [0.5000, 0.5000, 0.5000, 0.5000, 0.5000],\n",
       "        [0.5000, 0.5000, 0.5000, 0.5000, 0.5000],\n",
       "        [0.5000, 0.5000, 0.5000, 0.5000, 0.5000]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probability_matrix = torch.full((5,5), 0.5)\n",
    "probability_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False,  True, False],\n",
       "        [ True,  True,  True,  True, False],\n",
       "        [ True,  True, False, False,  True],\n",
       "        [ True, False,  True, False, False],\n",
       "        [ True,  True, False, False,  True]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "masked_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bos_token': '<s>',\n",
       " 'eos_token': '</s>',\n",
       " 'unk_token': '<unk>',\n",
       " 'sep_token': '</s>',\n",
       " 'pad_token': '<pad>',\n",
       " 'cls_token': '<s>',\n",
       " 'mask_token': '<mask>',\n",
       " 'additional_special_tokens': \"['<s>NOTUSED', '</s>NOTUSED', '‚ñÅ']\"}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "_data_collator = DataCollatorForSpanLevelMask(tokenizer=tokenizer,\n",
    "                                              mlm=True,\n",
    "                                              mlm_probability=0.15,\n",
    "                                              max_gram=3,\n",
    "                                              keep_prob=0.0,\n",
    "                                              mask_prob=1.0,\n",
    "                                              max_seq_len=510)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer,\n",
    "#                                               mlm=True,\n",
    "#                                               mlm_probability=0.15,\n",
    "#                                               max_gram=3,\n",
    "#                                               keep_prob=0.0,\n",
    "#                                               mask_prob=1.0,\n",
    "#                                               max_seq_len=510)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['‚ñÅ', '‡∏†‡∏≤‡∏©‡∏≤', '‡∏≠‡∏¥‡∏ô‡πÇ‡∏î', '-', '‡∏¢‡∏π‡πÇ‡∏£‡πÄ‡∏õ‡∏µ‡∏¢‡∏ô', '‡∏î‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏¥‡∏°', '‚ñÅ', '‡∏†‡∏≤‡∏©‡∏≤', '‡∏≠‡∏¥‡∏ô‡πÇ‡∏î', '-', '‡∏¢‡∏π‡πÇ‡∏£‡πÄ‡∏õ‡∏µ‡∏¢‡∏ô', '‡∏î‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏¥‡∏°', '‡∏Ñ‡∏∑‡∏≠', '‡∏†‡∏≤‡∏©‡∏≤', '‡∏î‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏¥‡∏°', '‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô', '‡∏ó‡∏µ‡πà‡∏™‡∏ô‡πÉ‡∏à', '‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î', '‚ñÅ', '‡∏£‡∏ß‡∏°‡∏ñ‡∏∂‡∏á', '‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à', '‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î', '‡∏≠‡∏µ‡∏Å‡∏î‡πâ‡∏ß‡∏¢', '‚ñÅ', '‡∏á‡∏≤‡∏ô', '‡∏™‡πà‡∏ß‡∏ô‡πÉ‡∏´‡∏ç‡πà', '‡∏Ç‡∏≠‡∏á‡∏ô‡∏±‡∏Å', '‡∏†‡∏≤‡∏©‡∏≤‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå', '‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á', '‡∏®‡∏ï‡∏ß‡∏£‡∏£‡∏©‡∏ó‡∏µ‡πà', '‚ñÅ', '19', '‚ñÅ', '‡∏°‡∏±‡∏Å‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô', '‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á', '‡∏ö‡∏π‡∏£‡∏ì‡∏∞', '‡∏†‡∏≤‡∏©‡∏≤‡∏ô‡∏µ‡πâ', '‚ñÅ', '‡πÅ‡∏•‡∏∞‡∏†‡∏≤‡∏©‡∏≤', '‡∏•‡∏π‡∏Å‡∏´‡∏•‡∏≤‡∏ô', '‚ñÅ', '‡πÄ‡∏ä‡πà‡∏ô', '‡∏†‡∏≤‡∏©‡∏≤', '‡πÄ‡∏à‡∏≠‡∏£‡πå‡πÅ‡∏°‡∏ô‡∏¥‡∏Å', '‡∏î‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏¥‡∏°'] 45\n"
     ]
    }
   ],
   "source": [
    "# text = \"We present SpanBERT, a pre-training method that is designed to better represent and predict spans of text.\"\n",
    "text = \"\"\"‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏¥‡∏ô‡πÇ‡∏î-‡∏¢‡∏π‡πÇ‡∏£‡πÄ‡∏õ‡∏µ‡∏¢‡∏ô‡∏î‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏¥‡∏° ‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏¥‡∏ô‡πÇ‡∏î-‡∏¢‡∏π‡πÇ‡∏£‡πÄ‡∏õ‡∏µ‡∏¢‡∏ô‡∏î‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏¥‡∏°‡∏Ñ‡∏∑‡∏≠‡∏†‡∏≤‡∏©‡∏≤‡∏î‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏¥‡∏°‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏ô‡πÉ‡∏à‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î \n",
    "‡∏£‡∏ß‡∏°‡∏ñ‡∏∂‡∏á‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏≠‡∏µ‡∏Å‡∏î‡πâ‡∏ß‡∏¢ ‡∏á‡∏≤‡∏ô‡∏™‡πà‡∏ß‡∏ô‡πÉ‡∏´‡∏ç‡πà‡∏Ç‡∏≠‡∏á‡∏ô‡∏±‡∏Å‡∏†‡∏≤‡∏©‡∏≤‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á‡∏®‡∏ï‡∏ß‡∏£‡∏£‡∏©‡∏ó‡∏µ‡πà 19¬†‡∏°‡∏±‡∏Å‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏ö‡∏π‡∏£‡∏ì‡∏∞‡∏†‡∏≤‡∏©‡∏≤‡∏ô‡∏µ‡πâ ‡πÅ‡∏•‡∏∞‡∏†‡∏≤‡∏©‡∏≤‡∏•‡∏π‡∏Å‡∏´‡∏•‡∏≤‡∏ô ‡πÄ‡∏ä‡πà‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÄ‡∏à‡∏≠‡∏£‡πå‡πÅ‡∏°‡∏ô‡∏¥‡∏Å‡∏î‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏¥‡∏°\"\"\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(tokens, len(tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([    5,     8,   213,  7203,    31, 12146,  1389,     8,   213,  7203,\n",
       "            31, 12146,  1389,    33,   213,  1389,   328,  8624,   484,     8,\n",
       "           383,  2477,   484,   535,     8,   166,   258,  6350,  8318,   254,\n",
       "           472,     8,   368,     8,  7828,    85,  3757, 11278,     8,  4979,\n",
       "          8452,     8,    61,   213, 23420,  1389,     6]),\n",
       " tensor([    5,     8,   213,  7203,    31, 12146,  1389,     8,   213,  7203,\n",
       "            31, 12146,  1389,    33,   213,  1389,   328,  8624,   484,     8,\n",
       "           383,  2477,   484,   535,     8,   166,   258,  6350,  8318,   254,\n",
       "           472,     8,   368,     8,  7828,    85,  3757, 11278,     8,  4979,\n",
       "          8452,     8,    61,   213, 23420,  1389,     6]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_1 = tokenizer.encode_plus(text, return_tensors='pt')['input_ids'].squeeze(0)\n",
    "inputs_2 = tokenizer.encode_plus(text, return_tensors='pt')['input_ids'].squeeze(0)\n",
    "inputs_1, inputs_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([    5,     8,   213,  7203,    31, 12146,  1389,     8,   213,  7203,\n",
       "            31, 12146,  1389,    33,   213,  1389,   328,  8624,   484,     8,\n",
       "           383,  2477,   484,   535,     8,   166,   258,  6350,  8318,   254,\n",
       "           472,     8,   368,     8,  7828,    85,  3757, 11278,     8,  4979,\n",
       "          8452,     8,    61,   213, 23420,  1389,     6]),\n",
       " tensor([    5,     8,   213,  7203,    31, 12146,  1389,     8,   213,  7203,\n",
       "            31, 12146,  1389,    33,   213,  1389,   328,  8624,   484,     8,\n",
       "           383,  2477,   484,   535,     8,   166,   258,  6350,  8318,   254,\n",
       "           472,     8,   368,     8,  7828,    85,  3757, 11278,     8,  4979,\n",
       "          8452,     8,    61,   213, 23420,  1389,     6]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(inputs_1, inputs_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.stack((inputs['input_ids'],), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'labels'])\n"
     ]
    }
   ],
   "source": [
    "res = _data_collator((inputs_1, inputs_2))\n",
    "print(res.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5, 8, 213, 7203, 31, 12146, 1389, 8, 24004, 24004, 31, 12146, 1389, 33, 213, 1389, 328, 8624, 24004, 8, 383, 2477, 484, 535, 8, 24004, 24004, 6350, 8318, 254, 472, 8, 368, 8, 7828, 85, 24004, 24004, 8, 4979, 8452, 8, 61, 213, 23420, 1389, 6], [5, 8, 213, 7203, 31, 24004, 1389, 8, 213, 24004, 31, 12146, 1389, 33, 213, 1389, 328, 8624, 24004, 8, 383, 2477, 484, 535, 8, 166, 258, 24004, 24004, 24004, 472, 8, 368, 8, 7828, 85, 3757, 11278, 8, 4979, 8452, 8, 24004, 213, 23420, 1389, 6]]\n"
     ]
    }
   ],
   "source": [
    "print(res['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-100, -100, -100, -100, -100, -100, -100, -100, 213, 7203, -100, -100, -100, -100, -100, -100, -100, -100, 484, -100, -100, -100, -100, -100, -100, 166, 258, -100, -100, -100, -100, -100, -100, -100, -100, -100, 3757, 11278, -100, -100, -100, -100, -100, -100, -100, -100, -100], [-100, -100, -100, -100, -100, 12146, -100, -100, -100, 7203, -100, -100, -100, -100, -100, -100, -100, -100, 484, -100, -100, -100, -100, -100, -100, -100, -100, 6350, 8318, 254, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 61, -100, -100, -100, -100]]\n"
     ]
    }
   ],
   "source": [
    "print(res['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profiling Data Collator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler\n",
    "%load_ext memory_profiler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, os\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "from torch.utils.data.sampler import RandomSampler, SequentialSampler\n",
    "\n",
    "\n",
    "from thai2transformers.datasets import MLMDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA_PATH = '../../dataset/split/thwiki-for-ddp_6.11.2020/train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../../dataset/split/thwiki-for-ddp_6.11.2020/train/train_debug.txt']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glob.glob(os.path.join(TRAIN_DATA_PATH, '*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   20000 ../../dataset/split/thwiki-for-ddp_6.11.2020/train/train_debug.txt\n"
     ]
    }
   ],
   "source": [
    "!wc -l ../../dataset/split/thwiki-for-ddp_6.11.2020/train/train_debug.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Build features (parallel).\n",
      "\n",
      "[INFO] Start groupping results.\n",
      "[INFO] Done.\n",
      "CPU times: user 324 ms, sys: 101 ms, total: 425 ms\n",
      "Wall time: 8.27 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "train_dataset = MLMDataset(tokenizer,\n",
    "                           TRAIN_DATA_PATH,\n",
    "                           510)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loader with bz=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sampler = SequentialSampler(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator_subword_mlm = DataCollatorForLanguageModeling(tokenizer,\n",
    "                                                        pad_to_multiple_of=8)\n",
    "\n",
    "data_loader_subword_mlm = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=8,\n",
    "            sampler=train_sampler,\n",
    "            collate_fn=data_collator_subword_mlm,\n",
    "            drop_last=False,\n",
    "            num_workers=0,\n",
    "            pin_memory=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator_span_mlm =  DataCollatorForSpanLevelMask(tokenizer=tokenizer,\n",
    "                                              mlm=True,\n",
    "                                              mlm_probability=0.15,\n",
    "                                              max_gram=3,\n",
    "                                              keep_prob=0.0,\n",
    "                                              mask_prob=1.0,\n",
    "                                              max_seq_len=510,\n",
    "                                              pad_to_multiple_of=8)\n",
    "\n",
    "\n",
    "data_loader_span_mlm = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=8,\n",
    "            sampler=train_sampler,\n",
    "            collate_fn=data_collator_span_mlm,\n",
    "            drop_last=False,\n",
    "            num_workers=0,\n",
    "            pin_memory=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........1.54 s ¬± 63.1 ms per loop (mean ¬± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "list(data_loader_subword_mlm)\n",
    "print('.', end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "text/plain": [
       "         529 function calls (526 primitive calls) in 0.002 seconds\n",
       "\n",
       "   Ordered by: internal time\n",
       "\n",
       "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
       "        1    0.000    0.000    0.001    0.001 data_collator.py:361(mask_tokens)\n",
       "        3    0.000    0.000    0.000    0.000 {built-in method bernoulli}\n",
       "        8    0.000    0.000    0.000    0.000 tokenization_utils_base.py:3128(<listcomp>)\n",
       "        1    0.000    0.000    0.000    0.000 data_collator.py:195(_collate_batch)\n",
       "        3    0.000    0.000    0.000    0.000 {method 'bool' of 'torch._C._TensorBase' objects}\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method empty}\n",
       "        3    0.000    0.000    0.000    0.000 {built-in method full}\n",
       "        1    0.000    0.000    0.000    0.000 {method 'tolist' of 'torch._C._TensorBase' objects}\n",
       "        8    0.000    0.000    0.000    0.000 tokenization_utils_base.py:1225(all_special_tokens_extended)\n",
       "       66    0.000    0.000    0.000    0.000 {method 'token_to_id' of 'tokenizers.Tokenizer' objects}\n",
       "        1    0.000    0.000    0.000    0.000 {method 'random_' of 'torch._C._TensorBase' objects}\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method tensor}\n",
       "       10    0.000    0.000    0.000    0.000 tokenization_utils_fast.py:220(convert_tokens_to_ids)\n",
       "        1    0.000    0.000    0.000    0.000 {method 'new_full' of 'torch._C._TensorBase' objects}\n",
       "        8    0.000    0.000    0.000    0.000 tokenization_utils_base.py:1222(<listcomp>)\n",
       "       64    0.000    0.000    0.000    0.000 {built-in method builtins.getattr}\n",
       "        8    0.000    0.000    0.000    0.000 tokenization_utils_base.py:1198(special_tokens_map_extended)\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method randint}\n",
       "        8    0.000    0.000    0.000    0.000 tokenization_utils_base.py:1241(all_special_ids)\n",
       "        8    0.000    0.000    0.000    0.000 {built-in method fromkeys}\n",
       "        1    0.000    0.000    0.002    0.002 {built-in method builtins.exec}\n",
       "       66    0.000    0.000    0.000    0.000 tokenization_utils_fast.py:242(_convert_token_to_id_with_added_voc)\n",
       "        1    0.000    0.000    0.000    0.000 {method 'clone' of 'torch._C._TensorBase' objects}\n",
       "        1    0.000    0.000    0.001    0.001 data_collator.py:372(<listcomp>)\n",
       "        1    0.000    0.000    0.000    0.000 dataloader.py:320(__init__)\n",
       "        1    0.000    0.000    0.002    0.002 data_collator.py:339(__call__)\n",
       "        1    0.000    0.000    0.000    0.000 {method 'masked_fill_' of 'torch._C._TensorBase' objects}\n",
       "       76    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}\n",
       "        8    0.000    0.000    0.000    0.000 tokenization_utils_base.py:3101(get_special_tokens_mask)\n",
       "       72    0.000    0.000    0.000    0.000 {method 'append' of 'list' objects}\n",
       "        1    0.000    0.000    0.000    0.000 {method 'item' of 'torch._C._TensorBase' objects}\n",
       "        8    0.000    0.000    0.000    0.000 tokenization_utils_base.py:1215(all_special_tokens)\n",
       "        2    0.000    0.000    0.000    0.000 sampler.py:198(__iter__)\n",
       "        1    0.000    0.000    0.002    0.002 <string>:1(<module>)\n",
       "        1    0.000    0.000    0.000    0.000 {method 'get_vocab_size' of 'tokenizers.Tokenizer' objects}\n",
       "        1    0.000    0.000    0.000    0.000 dataloader.py:375(__init__)\n",
       "        1    0.000    0.000    0.000    0.000 tokenization_utils_base.py:1010(mask_token)\n",
       "       11    0.000    0.000    0.000    0.000 {method 'size' of 'torch._C._TensorBase' objects}\n",
       "      3/2    0.000    0.000    0.000    0.000 {built-in method builtins.iter}\n",
       "    12/11    0.000    0.000    0.000    0.000 {built-in method builtins.len}\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method builtins.max}\n",
       "        1    0.000    0.000    0.002    0.002 dataloader.py:344(__next__)\n",
       "        1    0.000    0.000    0.000    0.000 fetch.py:44(<listcomp>)\n",
       "        1    0.000    0.000    0.002    0.002 fetch.py:42(fetch)\n",
       "        1    0.000    0.000    0.000    0.000 tokenization_utils_fast.py:153(__len__)\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method builtins.hasattr}\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method _abc._abc_instancecheck}\n",
       "        1    0.000    0.000    0.000    0.000 __init__.py:33(is_available)\n",
       "        1    0.000    0.000    0.000    0.000 sampler.py:61(__iter__)\n",
       "        1    0.000    0.000    0.000    0.000 dataloader.py:36(create_fetcher)\n",
       "        1    0.000    0.000    0.000    0.000 dataloader.py:275(__iter__)\n",
       "        1    0.000    0.000    0.002    0.002 dataloader.py:383(_next_data)\n",
       "        1    0.000    0.000    0.000    0.000 fetch.py:39(__init__)\n",
       "        1    0.000    0.000    0.000    0.000 tokenization_utils_base.py:1104(pad_token_id)\n",
       "        3    0.000    0.000    0.000    0.000 data_collator.py:203(<genexpr>)\n",
       "        9    0.000    0.000    0.000    0.000 data_collator.py:215(<genexpr>)\n",
       "        8    0.000    0.000    0.000    0.000 datasets.py:130(__getitem__)\n",
       "      2/1    0.000    0.000    0.002    0.002 {built-in method builtins.next}\n",
       "        1    0.000    0.000    0.000    0.000 abc.py:137(__instancecheck__)\n",
       "        2    0.000    0.000    0.000    0.000 dataloader.py:281(_auto_collation)\n",
       "        1    0.000    0.000    0.000    0.000 dataloader.py:338(_next_index)\n",
       "        1    0.000    0.000    0.000    0.000 fetch.py:8(__init__)\n",
       "        1    0.000    0.000    0.000    0.000 tokenization_utils_base.py:989(pad_token)\n",
       "        1    0.000    0.000    0.000    0.000 datasets.py:127(__len__)\n",
       "        1    0.000    0.000    0.000    0.000 {method 'pop' of 'dict' objects}\n",
       "        8    0.000    0.000    0.000    0.000 {method 'values' of 'dict' objects}\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method builtins.all}\n",
       "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
       "        1    0.000    0.000    0.000    0.000 dataloader.py:285(_index_sampler)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%prun next(iter(data_loader_subword_mlm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........20.1 s ¬± 672 ms per loop (mean ¬± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "list(data_loader_span_mlm)\n",
    "print('.', end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "text/plain": [
       "         10404 function calls (10401 primitive calls) in 0.014 seconds\n",
       "\n",
       "   Ordered by: internal time\n",
       "\n",
       "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
       "        8    0.004    0.001    0.010    0.001 tokenization_utils_fast.py:275(convert_ids_to_tokens)\n",
       "     1320    0.003    0.000    0.003    0.000 tensor.py:468(<lambda>)\n",
       "     1312    0.002    0.000    0.002    0.000 {method 'id_to_token' of 'tokenizers.Tokenizer' objects}\n",
       "        8    0.001    0.000    0.003    0.000 <ipython-input-20-f16f671911d4>:73(_mask_tokens)\n",
       "     1313    0.001    0.000    0.001    0.000 {method 'token_to_id' of 'tokenizers.Tokenizer' objects}\n",
       "        9    0.000    0.000    0.002    0.000 tokenization_utils_fast.py:220(convert_tokens_to_ids)\n",
       "     1313    0.000    0.000    0.001    0.000 tokenization_utils_fast.py:242(_convert_token_to_id_with_added_voc)\n",
       "     2648    0.000    0.000    0.000    0.000 {method 'append' of 'list' objects}\n",
       "        8    0.000    0.000    0.000    0.000 <ipython-input-20-f16f671911d4>:75(<listcomp>)\n",
       "      133    0.000    0.000    0.000    0.000 tokenization_utils_base.py:1010(mask_token)\n",
       "       86    0.000    0.000    0.000    0.000 {method 'cumsum' of 'numpy.ndarray' objects}\n",
       "       86    0.000    0.000    0.001    0.000 <ipython-input-20-f16f671911d4>:52(_choice)\n",
       "        1    0.000    0.000    0.000    0.000 {method 'item' of 'torch._C._TensorBase' objects}\n",
       "        8    0.000    0.000    0.000    0.000 <ipython-input-20-f16f671911d4>:103(<listcomp>)\n",
       "        1    0.000    0.000    0.014    0.014 <ipython-input-20-f16f671911d4>:107(mask_tokens)\n",
       "      133    0.000    0.000    0.000    0.000 <ipython-input-20-f16f671911d4>:58(_per_token_mask)\n",
       "       86    0.000    0.000    0.000    0.000 random.py:174(randrange)\n",
       "        1    0.000    0.000    0.000    0.000 data_collator.py:195(_collate_batch)\n",
       "       86    0.000    0.000    0.000    0.000 random.py:224(_randbelow)\n",
       "      180    0.000    0.000    0.000    0.000 {built-in method builtins.min}\n",
       "        8    0.000    0.000    0.000    0.000 {built-in method numpy.array}\n",
       "       86    0.000    0.000    0.000    0.000 random.py:218(randint)\n",
       "       86    0.000    0.000    0.000    0.000 fromnumeric.py:2405(cumsum)\n",
       "       86    0.000    0.000    0.000    0.000 <__array_function__ internals>:2(cumsum)\n",
       "        8    0.000    0.000    0.000    0.000 <ipython-input-20-f16f671911d4>:78(<listcomp>)\n",
       "       86    0.000    0.000    0.000    0.000 {built-in method _bisect.bisect_right}\n",
       "       86    0.000    0.000    0.000    0.000 fromnumeric.py:55(_wrapfunc)\n",
       "       95    0.000    0.000    0.000    0.000 {built-in method builtins.max}\n",
       "  309/308    0.000    0.000    0.000    0.000 {built-in method builtins.len}\n",
       "       86    0.000    0.000    0.000    0.000 {built-in method numpy.core._multiarray_umath.implement_array_function}\n",
       "      219    0.000    0.000    0.000    0.000 {method 'random' of '_random.Random' objects}\n",
       "        9    0.000    0.000    0.000    0.000 tensor.py:454(__iter__)\n",
       "        1    0.000    0.000    0.014    0.014 <string>:1(<module>)\n",
       "        8    0.000    0.000    0.000    0.000 {built-in method builtins.round}\n",
       "        1    0.000    0.000    0.014    0.014 {built-in method builtins.exec}\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method empty}\n",
       "       86    0.000    0.000    0.000    0.000 {built-in method builtins.getattr}\n",
       "      110    0.000    0.000    0.000    0.000 {method 'getrandbits' of '_random.Random' objects}\n",
       "        1    0.000    0.000    0.000    0.000 {method 'new_full' of 'torch._C._TensorBase' objects}\n",
       "        1    0.000    0.000    0.000    0.000 dataloader.py:320(__init__)\n",
       "        9    0.000    0.000    0.000    0.000 {built-in method torch._C._get_tracing_state}\n",
       "       20    0.000    0.000    0.000    0.000 {method 'size' of 'torch._C._TensorBase' objects}\n",
       "        1    0.000    0.000    0.014    0.014 data_collator.py:339(__call__)\n",
       "       86    0.000    0.000    0.000    0.000 fromnumeric.py:2401(_cumsum_dispatcher)\n",
       "        1    0.000    0.000    0.000    0.000 {method 'random_' of 'torch._C._TensorBase' objects}\n",
       "       86    0.000    0.000    0.000    0.000 {method 'bit_length' of 'int' objects}\n",
       "       19    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}\n",
       "        2    0.000    0.000    0.000    0.000 sampler.py:198(__iter__)\n",
       "    12/11    0.000    0.000    0.000    0.000 {built-in method builtins.iter}\n",
       "        1    0.000    0.000    0.000    0.000 dataloader.py:375(__init__)\n",
       "        1    0.000    0.000    0.014    0.014 dataloader.py:344(__next__)\n",
       "        1    0.000    0.000    0.014    0.014 fetch.py:42(fetch)\n",
       "        1    0.000    0.000    0.000    0.000 abc.py:137(__instancecheck__)\n",
       "        9    0.000    0.000    0.000    0.000 {method 'dim' of 'torch._C._TensorBase' objects}\n",
       "        1    0.000    0.000    0.000    0.000 sampler.py:61(__iter__)\n",
       "        1    0.000    0.000    0.000    0.000 dataloader.py:36(create_fetcher)\n",
       "        1    0.000    0.000    0.014    0.014 dataloader.py:383(_next_data)\n",
       "        1    0.000    0.000    0.000    0.000 tokenization_utils_base.py:1104(pad_token_id)\n",
       "        9    0.000    0.000    0.000    0.000 data_collator.py:215(<genexpr>)\n",
       "        8    0.000    0.000    0.000    0.000 datasets.py:130(__getitem__)\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method builtins.hasattr}\n",
       "        1    0.000    0.000    0.000    0.000 __init__.py:33(is_available)\n",
       "        1    0.000    0.000    0.000    0.000 dataloader.py:275(__iter__)\n",
       "        1    0.000    0.000    0.000    0.000 dataloader.py:338(_next_index)\n",
       "        1    0.000    0.000    0.000    0.000 fetch.py:39(__init__)\n",
       "        1    0.000    0.000    0.000    0.000 fetch.py:44(<listcomp>)\n",
       "        3    0.000    0.000    0.000    0.000 data_collator.py:203(<genexpr>)\n",
       "        1    0.000    0.000    0.000    0.000 {method 'pop' of 'dict' objects}\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method _abc._abc_instancecheck}\n",
       "        1    0.000    0.000    0.000    0.000 fetch.py:8(__init__)\n",
       "        1    0.000    0.000    0.000    0.000 tokenization_utils_base.py:989(pad_token)\n",
       "        1    0.000    0.000    0.000    0.000 datasets.py:127(__len__)\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method builtins.all}\n",
       "      2/1    0.000    0.000    0.014    0.014 {built-in method builtins.next}\n",
       "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
       "        2    0.000    0.000    0.000    0.000 dataloader.py:281(_auto_collation)\n",
       "        1    0.000    0.000    0.000    0.000 dataloader.py:285(_index_sampler)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%prun next(iter(data_loader_span_mlm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loader with bz=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "BZ=8\n",
    "\n",
    "data_collator_subword_mlm = DataCollatorForLanguageModeling(tokenizer,\n",
    "                                                        pad_to_multiple_of=8)\n",
    "\n",
    "data_loader_subword_mlm = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=BZ,\n",
    "            sampler=train_sampler,\n",
    "            collate_fn=data_collator_subword_mlm,\n",
    "            drop_last=False,\n",
    "            num_workers=0,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "data_collator_span_mlm =  DataCollatorForSpanLevelMask(tokenizer=tokenizer,\n",
    "                                              mlm=True,\n",
    "                                              mlm_probability=0.5,\n",
    "                                              max_gram=3,\n",
    "                                              keep_prob=0.0,\n",
    "                                              mask_prob=1.0,\n",
    "                                              max_seq_len=510,\n",
    "                                              pad_to_multiple_of=8)\n",
    "\n",
    "\n",
    "data_loader_span_mlm = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=BZ,\n",
    "            sampler=train_sampler,\n",
    "            collate_fn=data_collator_span_mlm,\n",
    "            drop_last=False,\n",
    "            num_workers=0,\n",
    "            pin_memory=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........1.66 s ¬± 57.2 ms per loop (mean ¬± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "list(data_loader_subword_mlm)\n",
    "print('.', end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........20.4 s ¬± 425 ms per loop (mean ¬± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "list(data_loader_span_mlm)\n",
    "print('.', end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "text/plain": [
       "         529 function calls (526 primitive calls) in 0.002 seconds\n",
       "\n",
       "   Ordered by: internal time\n",
       "\n",
       "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
       "        1    0.001    0.001    0.001    0.001 data_collator.py:361(mask_tokens)\n",
       "        8    0.000    0.000    0.000    0.000 tokenization_utils_base.py:3128(<listcomp>)\n",
       "        3    0.000    0.000    0.000    0.000 {built-in method bernoulli}\n",
       "        1    0.000    0.000    0.000    0.000 data_collator.py:195(_collate_batch)\n",
       "        1    0.000    0.000    0.000    0.000 {method 'item' of 'torch._C._TensorBase' objects}\n",
       "        8    0.000    0.000    0.000    0.000 tokenization_utils_base.py:1225(all_special_tokens_extended)\n",
       "        3    0.000    0.000    0.000    0.000 {method 'bool' of 'torch._C._TensorBase' objects}\n",
       "        3    0.000    0.000    0.000    0.000 {built-in method full}\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method tensor}\n",
       "       10    0.000    0.000    0.000    0.000 tokenization_utils_fast.py:220(convert_tokens_to_ids)\n",
       "        1    0.000    0.000    0.000    0.000 {method 'tolist' of 'torch._C._TensorBase' objects}\n",
       "       66    0.000    0.000    0.000    0.000 {method 'token_to_id' of 'tokenizers.Tokenizer' objects}\n",
       "        8    0.000    0.000    0.000    0.000 tokenization_utils_base.py:1198(special_tokens_map_extended)\n",
       "        8    0.000    0.000    0.000    0.000 tokenization_utils_base.py:1222(<listcomp>)\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method randint}\n",
       "        1    0.000    0.000    0.000    0.000 {method 'new_full' of 'torch._C._TensorBase' objects}\n",
       "        1    0.000    0.000    0.002    0.002 {built-in method builtins.exec}\n",
       "       64    0.000    0.000    0.000    0.000 {built-in method builtins.getattr}\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method empty}\n",
       "        1    0.000    0.000    0.000    0.000 dataloader.py:320(__init__)\n",
       "       66    0.000    0.000    0.000    0.000 tokenization_utils_fast.py:242(_convert_token_to_id_with_added_voc)\n",
       "        8    0.000    0.000    0.000    0.000 {built-in method fromkeys}\n",
       "        1    0.000    0.000    0.002    0.002 data_collator.py:339(__call__)\n",
       "       76    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}\n",
       "        1    0.000    0.000    0.000    0.000 data_collator.py:372(<listcomp>)\n",
       "        1    0.000    0.000    0.000    0.000 {method 'clone' of 'torch._C._TensorBase' objects}\n",
       "       72    0.000    0.000    0.000    0.000 {method 'append' of 'list' objects}\n",
       "        1    0.000    0.000    0.000    0.000 {method 'random_' of 'torch._C._TensorBase' objects}\n",
       "        1    0.000    0.000    0.000    0.000 {method 'masked_fill_' of 'torch._C._TensorBase' objects}\n",
       "        2    0.000    0.000    0.000    0.000 sampler.py:198(__iter__)\n",
       "        8    0.000    0.000    0.000    0.000 tokenization_utils_base.py:3101(get_special_tokens_mask)\n",
       "       11    0.000    0.000    0.000    0.000 {method 'size' of 'torch._C._TensorBase' objects}\n",
       "        1    0.000    0.000    0.002    0.002 <string>:1(<module>)\n",
       "        8    0.000    0.000    0.000    0.000 tokenization_utils_base.py:1215(all_special_tokens)\n",
       "        1    0.000    0.000    0.000    0.000 {method 'get_vocab_size' of 'tokenizers.Tokenizer' objects}\n",
       "        1    0.000    0.000    0.000    0.000 tokenization_utils_base.py:1010(mask_token)\n",
       "        8    0.000    0.000    0.000    0.000 tokenization_utils_base.py:1241(all_special_ids)\n",
       "        1    0.000    0.000    0.000    0.000 dataloader.py:375(__init__)\n",
       "        8    0.000    0.000    0.000    0.000 {method 'values' of 'dict' objects}\n",
       "        1    0.000    0.000    0.000    0.000 sampler.py:61(__iter__)\n",
       "        1    0.000    0.000    0.000    0.000 dataloader.py:36(create_fetcher)\n",
       "        1    0.000    0.000    0.002    0.002 dataloader.py:383(_next_data)\n",
       "        1    0.000    0.000    0.000    0.000 tokenization_utils_fast.py:153(__len__)\n",
       "        8    0.000    0.000    0.000    0.000 datasets.py:130(__getitem__)\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method builtins.hasattr}\n",
       "    12/11    0.000    0.000    0.000    0.000 {built-in method builtins.len}\n",
       "        1    0.000    0.000    0.002    0.002 dataloader.py:344(__next__)\n",
       "      3/2    0.000    0.000    0.000    0.000 {built-in method builtins.iter}\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method builtins.max}\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method _abc._abc_instancecheck}\n",
       "        1    0.000    0.000    0.000    0.000 abc.py:137(__instancecheck__)\n",
       "        1    0.000    0.000    0.000    0.000 __init__.py:33(is_available)\n",
       "        1    0.000    0.000    0.000    0.000 dataloader.py:275(__iter__)\n",
       "        1    0.000    0.000    0.000    0.000 fetch.py:8(__init__)\n",
       "        1    0.000    0.000    0.000    0.000 fetch.py:39(__init__)\n",
       "        1    0.000    0.000    0.000    0.000 fetch.py:44(<listcomp>)\n",
       "        1    0.000    0.000    0.002    0.002 fetch.py:42(fetch)\n",
       "        1    0.000    0.000    0.000    0.000 tokenization_utils_base.py:989(pad_token)\n",
       "        1    0.000    0.000    0.000    0.000 tokenization_utils_base.py:1104(pad_token_id)\n",
       "        1    0.000    0.000    0.000    0.000 {method 'pop' of 'dict' objects}\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method builtins.all}\n",
       "      2/1    0.000    0.000    0.002    0.002 {built-in method builtins.next}\n",
       "        1    0.000    0.000    0.000    0.000 dataloader.py:285(_index_sampler)\n",
       "        1    0.000    0.000    0.000    0.000 dataloader.py:338(_next_index)\n",
       "        3    0.000    0.000    0.000    0.000 data_collator.py:203(<genexpr>)\n",
       "        9    0.000    0.000    0.000    0.000 data_collator.py:215(<genexpr>)\n",
       "        1    0.000    0.000    0.000    0.000 datasets.py:127(__len__)\n",
       "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
       "        2    0.000    0.000    0.000    0.000 dataloader.py:281(_auto_collation)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%prun next(iter(data_loader_subword_mlm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "text/plain": [
       "         10286 function calls (10283 primitive calls) in 0.016 seconds\n",
       "\n",
       "   Ordered by: internal time\n",
       "\n",
       "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
       "        8    0.005    0.001    0.011    0.001 tokenization_utils_fast.py:275(convert_ids_to_tokens)\n",
       "     1320    0.003    0.000    0.003    0.000 tensor.py:468(<lambda>)\n",
       "     1312    0.002    0.000    0.002    0.000 {method 'id_to_token' of 'tokenizers.Tokenizer' objects}\n",
       "        8    0.001    0.000    0.003    0.000 <ipython-input-20-f16f671911d4>:73(_mask_tokens)\n",
       "     1313    0.001    0.000    0.001    0.000 {method 'token_to_id' of 'tokenizers.Tokenizer' objects}\n",
       "        9    0.000    0.000    0.002    0.000 tokenization_utils_fast.py:220(convert_tokens_to_ids)\n",
       "     1313    0.000    0.000    0.001    0.000 tokenization_utils_fast.py:242(_convert_token_to_id_with_added_voc)\n",
       "     2648    0.000    0.000    0.000    0.000 {method 'append' of 'list' objects}\n",
       "        8    0.000    0.000    0.000    0.000 <ipython-input-20-f16f671911d4>:75(<listcomp>)\n",
       "      131    0.000    0.000    0.000    0.000 tokenization_utils_base.py:1010(mask_token)\n",
       "       80    0.000    0.000    0.001    0.000 <ipython-input-20-f16f671911d4>:52(_choice)\n",
       "       80    0.000    0.000    0.000    0.000 {method 'cumsum' of 'numpy.ndarray' objects}\n",
       "      131    0.000    0.000    0.000    0.000 <ipython-input-20-f16f671911d4>:58(_per_token_mask)\n",
       "        8    0.000    0.000    0.000    0.000 <ipython-input-20-f16f671911d4>:103(<listcomp>)\n",
       "        1    0.000    0.000    0.015    0.015 <ipython-input-20-f16f671911d4>:107(mask_tokens)\n",
       "        1    0.000    0.000    0.000    0.000 data_collator.py:195(_collate_batch)\n",
       "       80    0.000    0.000    0.000    0.000 random.py:174(randrange)\n",
       "       80    0.000    0.000    0.000    0.000 random.py:224(_randbelow)\n",
       "        8    0.000    0.000    0.000    0.000 {built-in method numpy.array}\n",
       "       80    0.000    0.000    0.000    0.000 fromnumeric.py:2405(cumsum)\n",
       "        8    0.000    0.000    0.000    0.000 <ipython-input-20-f16f671911d4>:78(<listcomp>)\n",
       "      168    0.000    0.000    0.000    0.000 {built-in method builtins.min}\n",
       "        1    0.000    0.000    0.000    0.000 {method 'new_full' of 'torch._C._TensorBase' objects}\n",
       "       80    0.000    0.000    0.000    0.000 random.py:218(randint)\n",
       "       80    0.000    0.000    0.000    0.000 <__array_function__ internals>:2(cumsum)\n",
       "       80    0.000    0.000    0.000    0.000 {built-in method _bisect.bisect_right}\n",
       "       80    0.000    0.000    0.000    0.000 fromnumeric.py:55(_wrapfunc)\n",
       "       80    0.000    0.000    0.000    0.000 {built-in method numpy.core._multiarray_umath.implement_array_function}\n",
       "        9    0.000    0.000    0.000    0.000 tensor.py:454(__iter__)\n",
       "        1    0.000    0.000    0.016    0.016 <string>:1(<module>)\n",
       "  291/290    0.000    0.000    0.000    0.000 {built-in method builtins.len}\n",
       "       89    0.000    0.000    0.000    0.000 {built-in method builtins.max}\n",
       "      118    0.000    0.000    0.000    0.000 {method 'getrandbits' of '_random.Random' objects}\n",
       "       20    0.000    0.000    0.000    0.000 {method 'size' of 'torch._C._TensorBase' objects}\n",
       "        1    0.000    0.000    0.016    0.016 {built-in method builtins.exec}\n",
       "       80    0.000    0.000    0.000    0.000 {built-in method builtins.getattr}\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method empty}\n",
       "        1    0.000    0.000    0.000    0.000 {method 'item' of 'torch._C._TensorBase' objects}\n",
       "        9    0.000    0.000    0.000    0.000 {built-in method torch._C._get_tracing_state}\n",
       "      211    0.000    0.000    0.000    0.000 {method 'random' of '_random.Random' objects}\n",
       "        1    0.000    0.000    0.000    0.000 dataloader.py:320(__init__)\n",
       "        1    0.000    0.000    0.016    0.016 data_collator.py:339(__call__)\n",
       "       80    0.000    0.000    0.000    0.000 fromnumeric.py:2401(_cumsum_dispatcher)\n",
       "        1    0.000    0.000    0.000    0.000 {method 'random_' of 'torch._C._TensorBase' objects}\n",
       "       19    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}\n",
       "        8    0.000    0.000    0.000    0.000 {built-in method builtins.round}\n",
       "       80    0.000    0.000    0.000    0.000 {method 'bit_length' of 'int' objects}\n",
       "        2    0.000    0.000    0.000    0.000 sampler.py:198(__iter__)\n",
       "        9    0.000    0.000    0.000    0.000 {method 'dim' of 'torch._C._TensorBase' objects}\n",
       "    12/11    0.000    0.000    0.000    0.000 {built-in method builtins.iter}\n",
       "        1    0.000    0.000    0.000    0.000 dataloader.py:375(__init__)\n",
       "        1    0.000    0.000    0.000    0.000 dataloader.py:36(create_fetcher)\n",
       "        1    0.000    0.000    0.016    0.016 fetch.py:42(fetch)\n",
       "        9    0.000    0.000    0.000    0.000 data_collator.py:215(<genexpr>)\n",
       "        8    0.000    0.000    0.000    0.000 datasets.py:130(__getitem__)\n",
       "        1    0.000    0.000    0.000    0.000 abc.py:137(__instancecheck__)\n",
       "        1    0.000    0.000    0.000    0.000 sampler.py:61(__iter__)\n",
       "        1    0.000    0.000    0.000    0.000 dataloader.py:275(__iter__)\n",
       "        1    0.000    0.000    0.016    0.016 dataloader.py:344(__next__)\n",
       "        1    0.000    0.000    0.016    0.016 dataloader.py:383(_next_data)\n",
       "        1    0.000    0.000    0.000    0.000 tokenization_utils_base.py:1104(pad_token_id)\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method builtins.hasattr}\n",
       "        1    0.000    0.000    0.000    0.000 __init__.py:33(is_available)\n",
       "        1    0.000    0.000    0.000    0.000 dataloader.py:285(_index_sampler)\n",
       "        1    0.000    0.000    0.000    0.000 dataloader.py:338(_next_index)\n",
       "        1    0.000    0.000    0.000    0.000 fetch.py:39(__init__)\n",
       "        1    0.000    0.000    0.000    0.000 fetch.py:44(<listcomp>)\n",
       "        3    0.000    0.000    0.000    0.000 data_collator.py:203(<genexpr>)\n",
       "      2/1    0.000    0.000    0.016    0.016 {built-in method builtins.next}\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method _abc._abc_instancecheck}\n",
       "        2    0.000    0.000    0.000    0.000 dataloader.py:281(_auto_collation)\n",
       "        1    0.000    0.000    0.000    0.000 fetch.py:8(__init__)\n",
       "        1    0.000    0.000    0.000    0.000 tokenization_utils_base.py:989(pad_token)\n",
       "        1    0.000    0.000    0.000    0.000 datasets.py:127(__len__)\n",
       "        1    0.000    0.000    0.000    0.000 {method 'pop' of 'dict' objects}\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method builtins.all}\n",
       "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%prun next(iter(data_loader_span_mlm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
