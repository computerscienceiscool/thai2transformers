{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filename: refactor_span_level_data_collator_v3.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler\n",
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../thai2transformers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer , DataCollatorForLanguageModeling\n",
    "import math\n",
    "from typing import List, Dict, Union, Optional, Tuple, Any\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "from bisect import bisect\n",
    "from transformers.data.data_collator import DataCollatorForLanguageModeling, _collate_batch, tolist\n",
    "from transformers.tokenization_utils_base import BatchEncoding, PreTrainedTokenizerBase\n",
    "\n",
    "import glob, os\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "from torch.utils.data.sampler import RandomSampler, SequentialSampler\n",
    "\n",
    "\n",
    "from thai2transformers.datasets import MLMDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.6.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('airesearchth/wangchanberta-base-wiki-20210520-spm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='airesearchth/wangchanberta-base-wiki-20210520-spm', vocab_size=24005, model_max_len=1000000000000000019884624838656, is_fast=True, padding_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True), 'additional_special_tokens': ['<s>NOTUSED', '</s>NOTUSED', '▁']})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA_PATH = '../../dataset/split/thwiki-for-ddp_6.11.2020/train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_LARGE_DATA_PATH = '../../dataset/split/thwiki-for-ddp_concat_12.11.2020/train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ม้าลายเบอร์เชลล์<_>เป็นม้าลายชนิดย่อยหรือสปีชีส์ย่อยของม้าลายธรรมดา<_>(\"E.<_>quagga\")<_>ชนิดหนึ่ง\n",
      "ม้าลายเบอร์เชลล์<_>เป็นม้าลายที่แพร่กระจายพันธุ์ในทวีปแอฟริกาตอนใต้<_>เช่น<_>บอตสวานา,<_>สวาซิแลนด์,<_>แอฟริกาใต้<_>เป็นต้น<_>เป็นม้าลายที่มีลายขนาดใหญ่สีดำพาดยาวสลับกับลายสีขาวจากหลังลงไปทั้งสองข้างของลำตัวจนถึงใต้ท้อง<_>มีพฤติกรรมและลักษณะนิสัยคล้ายกับม้าลายธรรมดาทั่วไป\n"
     ]
    }
   ],
   "source": [
    "!head -2 $TRAIN_LARGE_DATA_PATH/train.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Build features (parallel).\n",
      "\n",
      "[INFO] Start groupping results.\n",
      "[INFO] Done.\n",
      "CPU times: user 326 ms, sys: 112 ms, total: 437 ms\n",
      "Wall time: 2.36 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "train_dataset = MLMDataset(tokenizer,\n",
    "                           TRAIN_DATA_PATH,\n",
    "                           510)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Build features (parallel).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-8:\n",
      "Process ForkPoolWorker-9:\n",
      "Process ForkPoolWorker-10:\n",
      "Process ForkPoolWorker-12:\n",
      "Process ForkPoolWorker-11:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/project/vistec-AI/airesearch-ner/vistec-ner-tagger/submodules/thai2transformers/thai2transformers/datasets.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, tokenizer, data_dir, max_length, binarized_path, ext, bs, parallelize, chunksize, chunk_process)\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mparallelize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_parallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_process\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/project/vistec-AI/airesearch-ner/vistec-ner-tagger/submodules/thai2transformers/thai2transformers/datasets.py\u001b[0m in \u001b[0;36m_build_parallel\u001b[0;34m(self, chunk_process)\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'[INFO] Build features (parallel).'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mmultiprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_cores\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m                 \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_one\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'[INFO] Start groupping results.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlst\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlst\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/saiko/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mthat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mreturned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         '''\n\u001b[0;32m--> 268\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapstar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstarmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/saiko/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 651\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    652\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/saiko/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 648\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/saiko/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/saiko/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "train_dataset_large = MLMDataset(tokenizer,\n",
    "                           TRAIN_LARGE_DATA_PATH,\n",
    "                           100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SPECIAL_TOKEN_NAMES = ['bos_token', 'eos_token', 'sep_token', 'cls_token', 'pad_token']\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForSpanLevelMask(DataCollatorForLanguageModeling):\n",
    "    \"\"\"\n",
    "    Data collator used for span-level masked language modeling\n",
    "     \n",
    "    adapted from NGramMaskGenerator class\n",
    "    \n",
    "    https://github.com/microsoft/DeBERTa/blob/11fa20141d9700ba2272b38f2d5fce33d981438b/DeBERTa/apps/tasks/mlm_task.py#L36\n",
    "    and\n",
    "    https://github.com/zihangdai/xlnet/blob/0b642d14dd8aec7f1e1ecbf7d6942d5faa6be1f0/data_utils.py\n",
    "\n",
    "    \"\"\"\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    mlm: bool = True\n",
    "    mlm_probability: float = 0.15\n",
    "    max_gram: int = 3\n",
    "    keep_prob: float = 0.0\n",
    "    mask_prob: float = 1.0\n",
    "    max_preds_per_seq: int = None\n",
    "    max_seq_len: int = 510\n",
    "\n",
    "    def __init__(self, tokenizer, mlm=True, mlm_probability=0.15, *args, **kwargs):\n",
    "        super().__init__(tokenizer, mlm=mlm, mlm_probability=mlm_probability)\n",
    "\n",
    "        assert self.mask_prob + self.keep_prob <= 1, \\\n",
    "            f'The prob of using [MASK]({self.mask_prob}) and the prob of using original token({self.keep_prob}) should between [0,1]'\n",
    "\n",
    "        if self.max_preds_per_seq is None:\n",
    "            self.max_preds_per_seq = math.ceil(self.max_seq_len * self.mlm_probability / 10) * 10\n",
    "            self.mask_window = int(1 / self.mlm_probability) # make ngrams per window sized context\n",
    "        self.vocab_words = list(self.tokenizer.get_vocab().keys())\n",
    "        self.vocab_mapping = self.tokenizer.get_vocab()\n",
    "        \n",
    "        self.special_tokens = [self.tokenizer.special_tokens_map[name] for name in  SPECIAL_TOKEN_NAMES]\n",
    "#         print(' self.special_tokens', self.special_tokens)\n",
    "        self.ngrams = np.arange(1, self.max_gram + 1, dtype=np.int64)\n",
    "        _pvals = 1. / np.arange(1, self.max_gram + 1)\n",
    "        self.pvals = _pvals / _pvals.sum(keepdims=True)\n",
    "        print('max_gram', self.max_gram)\n",
    "    def _choice(self, rng, data, p):\n",
    "        cul = np.cumsum(p)\n",
    "        x = rng.random()*cul[-1]\n",
    "        id = bisect(cul, x)\n",
    "        return data[id]\n",
    "\n",
    "    def _per_token_mask(self, idx, tokens, rng, mask_prob, keep_prob):\n",
    "        label = tokens[idx]\n",
    "        mask = self.tokenizer.mask_token\n",
    "        rand = rng.random()\n",
    "        if rand < mask_prob:\n",
    "            new_label = mask\n",
    "        elif rand < mask_prob + keep_prob:\n",
    "            new_label = label\n",
    "        else:\n",
    "            new_label = rng.choice(self.vocab_words)\n",
    "\n",
    "        tokens[idx] = new_label\n",
    "\n",
    "        return label\n",
    "\n",
    "    def _mask_tokens(self, tokens: List[str], rng=random, **kwargs):\n",
    "\n",
    "        indices = [i for i in range(len(tokens)) if tokens[i] not in self.special_tokens]\n",
    "#         print('debug: indices to be able to be masked', indices)\n",
    "        \n",
    "        unigrams = [ [idx] for idx in indices ]\n",
    "        num_to_predict = min(self.max_preds_per_seq, max(1, int(round(len(tokens) * self.mlm_probability))))\n",
    "           \n",
    "        offset = 0\n",
    "        mask_grams = np.array([False]*len(unigrams))\n",
    "        while offset < len(unigrams):\n",
    "            n = self._choice(rng, self.ngrams, p=self.pvals)\n",
    "            ctx_size = min(n * self.mask_window, len(unigrams)-offset)\n",
    "            m = rng.randint(0, ctx_size-1)\n",
    "            s = offset + m\n",
    "            e = min(offset + m + n, len(unigrams))\n",
    "            offset = max(offset+ctx_size, e)\n",
    "            mask_grams[s:e] = True\n",
    "\n",
    "        target_labels = [None]*len(tokens)\n",
    "        w_cnt = 0\n",
    "        for m,word in zip(mask_grams, unigrams):\n",
    "            if m:\n",
    "                for idx in word:\n",
    "                    label = self._per_token_mask(idx, tokens, rng, self.mask_prob, self.keep_prob)\n",
    "                    target_labels[idx] = label\n",
    "                    w_cnt += 1\n",
    "                if w_cnt >= num_to_predict:\n",
    "                    break\n",
    "\n",
    "        target_labels = [self.vocab_mapping[x] if x else -100 for x in target_labels]\n",
    "        return tokens, target_labels    \n",
    "\n",
    "\n",
    "    def mask_tokens(\n",
    "        self, inputs: torch.Tensor, special_tokens_mask: Optional[torch.Tensor] = None\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original.\n",
    "        \"\"\"\n",
    "        labels = []\n",
    "        # We sample a few tokens in each sequence for MLM training (with probability `self.mlm_probabilityability`)\n",
    "        # probability_matrix = torch.full(labels.shape, self.mlm_probabilityability)\n",
    "        # if special_tokens_mask is None:\n",
    "        #     special_tokens_mask = [\n",
    "        #         self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()\n",
    "        #     ]\n",
    "        #     special_tokens_mask = torch.tensor(special_tokens_mask, dtype=torch.bool)\n",
    "        # else:\n",
    "        #     special_tokens_mask = special_tokens_mask.bool()\n",
    "\n",
    "#         print('inputs', inputs.shape, inputs)\n",
    "        inputs_masked = []\n",
    "        \n",
    "        for i, input in enumerate(inputs):\n",
    "#             print('input',input)\n",
    "            input_tokens = self.tokenizer.convert_ids_to_tokens(input)\n",
    "            \n",
    "\n",
    "            input_masked, _labels = self._mask_tokens(input_tokens)\n",
    "#             print('DEBUG: input_masked', input_masked)\n",
    "            input_masked_ids = self.tokenizer.convert_tokens_to_ids(input_masked)\n",
    "            inputs_masked.append(input_masked_ids)\n",
    "#             print('_labels, ', _labels)\n",
    "#             print('inputs_masked, ', input_masked_ids)\n",
    "            labels.append(_labels)\n",
    "      \n",
    "        return inputs_masked, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sampler = SequentialSampler(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_gram 3\n"
     ]
    }
   ],
   "source": [
    "BZ=32\n",
    "\n",
    "data_collator_span_mlm =  DataCollatorForSpanLevelMask(tokenizer=tokenizer,\n",
    "                                              mlm=True,\n",
    "                                              mlm_probability=0.15,\n",
    "                                              max_gram=3,\n",
    "                                              keep_prob=0.0,\n",
    "                                              mask_prob=1.0,\n",
    "                                              max_seq_len=510,\n",
    "                                              pad_to_multiple_of=8)\n",
    "\n",
    "data_loader_span_mlm = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=BZ,\n",
    "            sampler=train_sampler,\n",
    "            collate_fn=data_collator_span_mlm,\n",
    "            drop_last=False,\n",
    "            num_workers=0,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "data_collator_subword_mlm =  DataCollatorForLanguageModeling(tokenizer=tokenizer,\n",
    "                                              mlm=True,\n",
    "                                              mlm_probability=0.15,\n",
    "                                              pad_to_multiple_of=8)\n",
    "\n",
    "data_loader_subword_mlm = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=BZ,\n",
    "            sampler=train_sampler,\n",
    "            collate_fn=data_collator_subword_mlm,\n",
    "            drop_last=False,\n",
    "            num_workers=0,\n",
    "            pin_memory=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "text/plain": [
       "         63089 function calls (63086 primitive calls) in 0.073 seconds\n",
       "\n",
       "   Ordered by: internal time\n",
       "\n",
       "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
       "     8960    0.019    0.000    0.019    0.000 tensor.py:468(<lambda>)\n",
       "       32    0.018    0.001    0.052    0.002 tokenization_utils_fast.py:275(convert_ids_to_tokens)\n",
       "     8928    0.014    0.000    0.014    0.000 {method 'id_to_token' of 'tokenizers.Tokenizer' objects}\n",
       "       33    0.003    0.000    0.009    0.000 tokenization_utils_fast.py:220(convert_tokens_to_ids)\n",
       "     8929    0.003    0.000    0.003    0.000 {method 'token_to_id' of 'tokenizers.Tokenizer' objects}\n",
       "       32    0.003    0.000    0.010    0.000 <ipython-input-9-1161ad0796fc>:63(_mask_tokens)\n",
       "     8929    0.002    0.000    0.005    0.000 tokenization_utils_fast.py:242(_convert_token_to_id_with_added_voc)\n",
       "    17952    0.002    0.000    0.002    0.000 {method 'append' of 'list' objects}\n",
       "       32    0.001    0.000    0.001    0.000 <ipython-input-9-1161ad0796fc>:65(<listcomp>)\n",
       "      524    0.001    0.000    0.001    0.000 tokenization_utils_base.py:1010(mask_token)\n",
       "      323    0.001    0.000    0.001    0.000 {method 'cumsum' of 'numpy.ndarray' objects}\n",
       "      323    0.001    0.000    0.002    0.000 <ipython-input-9-1161ad0796fc>:42(_choice)\n",
       "       32    0.001    0.000    0.001    0.000 <ipython-input-9-1161ad0796fc>:93(<listcomp>)\n",
       "        1    0.000    0.000    0.072    0.072 <ipython-input-9-1161ad0796fc>:97(mask_tokens)\n",
       "        1    0.000    0.000    0.001    0.001 data_collator.py:195(_collate_batch)\n",
       "      524    0.000    0.000    0.001    0.000 <ipython-input-9-1161ad0796fc>:48(_per_token_mask)\n",
       "      323    0.000    0.000    0.001    0.000 random.py:174(randrange)\n",
       "      678    0.000    0.000    0.000    0.000 {built-in method builtins.min}\n",
       "       32    0.000    0.000    0.000    0.000 {built-in method numpy.array}\n",
       "      323    0.000    0.000    0.000    0.000 random.py:224(_randbelow)\n",
       "        1    0.000    0.000    0.000    0.000 {method 'random_' of 'torch._C._TensorBase' objects}\n",
       "      323    0.000    0.000    0.001    0.000 fromnumeric.py:2405(cumsum)\n",
       "      323    0.000    0.000    0.002    0.000 <__array_function__ internals>:2(cumsum)\n",
       "      323    0.000    0.000    0.001    0.000 random.py:218(randint)\n",
       "       32    0.000    0.000    0.000    0.000 <ipython-input-9-1161ad0796fc>:68(<listcomp>)\n",
       "      323    0.000    0.000    0.000    0.000 {built-in method _bisect.bisect_right}\n",
       "        1    0.000    0.000    0.000    0.000 {method 'new_full' of 'torch._C._TensorBase' objects}\n",
       "      323    0.000    0.000    0.001    0.000 fromnumeric.py:55(_wrapfunc)\n",
       "      323    0.000    0.000    0.001    0.000 {built-in method numpy.core._multiarray_umath.implement_array_function}\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method empty}\n",
       "       33    0.000    0.000    0.000    0.000 tensor.py:454(__iter__)\n",
       "        1    0.000    0.000    0.000    0.000 {method 'item' of 'torch._C._TensorBase' objects}\n",
       "      847    0.000    0.000    0.000    0.000 {method 'random' of '_random.Random' objects}\n",
       "1164/1163    0.000    0.000    0.000    0.000 {built-in method builtins.len}\n",
       "      356    0.000    0.000    0.000    0.000 {built-in method builtins.max}\n",
       "        1    0.000    0.000    0.001    0.001 dataloader.py:320(__init__)\n",
       "      463    0.000    0.000    0.000    0.000 {method 'getrandbits' of '_random.Random' objects}\n",
       "        1    0.000    0.000    0.073    0.073 <string>:1(<module>)\n",
       "       68    0.000    0.000    0.000    0.000 {method 'size' of 'torch._C._TensorBase' objects}\n",
       "      323    0.000    0.000    0.000    0.000 fromnumeric.py:2401(_cumsum_dispatcher)\n",
       "      323    0.000    0.000    0.000    0.000 {built-in method builtins.getattr}\n",
       "       33    0.000    0.000    0.000    0.000 {built-in method torch._C._get_tracing_state}\n",
       "      323    0.000    0.000    0.000    0.000 {method 'bit_length' of 'int' objects}\n",
       "       67    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}\n",
       "       32    0.000    0.000    0.000    0.000 {built-in method builtins.round}\n",
       "        1    0.000    0.000    0.072    0.072 dataloader.py:344(__next__)\n",
       "        1    0.000    0.000    0.000    0.000 abc.py:141(__subclasscheck__)\n",
       "        1    0.000    0.000    0.073    0.073 {built-in method builtins.exec}\n",
       "        1    0.000    0.000    0.072    0.072 data_collator.py:339(__call__)\n",
       "       33    0.000    0.000    0.000    0.000 {method 'dim' of 'torch._C._TensorBase' objects}\n",
       "        2    0.000    0.000    0.000    0.000 sampler.py:198(__iter__)\n",
       "    36/35    0.000    0.000    0.001    0.000 {built-in method builtins.iter}\n",
       "        1    0.000    0.000    0.000    0.000 fetch.py:44(<listcomp>)\n",
       "       33    0.000    0.000    0.000    0.000 data_collator.py:215(<genexpr>)\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method _abc._abc_subclasscheck}\n",
       "        1    0.000    0.000    0.001    0.001 dataloader.py:375(__init__)\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method _abc._abc_instancecheck}\n",
       "        1    0.000    0.000    0.000    0.000 fetch.py:8(__init__)\n",
       "       32    0.000    0.000    0.000    0.000 datasets.py:130(__getitem__)\n",
       "        1    0.000    0.000    0.000    0.000 sampler.py:61(__iter__)\n",
       "        1    0.000    0.000    0.000    0.000 dataloader.py:36(create_fetcher)\n",
       "        1    0.000    0.000    0.072    0.072 dataloader.py:383(_next_data)\n",
       "        1    0.000    0.000    0.072    0.072 fetch.py:42(fetch)\n",
       "        1    0.000    0.000    0.000    0.000 tokenization_utils_base.py:1104(pad_token_id)\n",
       "        1    0.000    0.000    0.000    0.000 __init__.py:33(is_available)\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method builtins.hasattr}\n",
       "        1    0.000    0.000    0.000    0.000 abc.py:137(__instancecheck__)\n",
       "        1    0.000    0.000    0.001    0.001 dataloader.py:275(__iter__)\n",
       "        1    0.000    0.000    0.000    0.000 dataloader.py:338(_next_index)\n",
       "        1    0.000    0.000    0.000    0.000 fetch.py:39(__init__)\n",
       "        3    0.000    0.000    0.000    0.000 data_collator.py:203(<genexpr>)\n",
       "        1    0.000    0.000    0.000    0.000 {method 'pop' of 'dict' objects}\n",
       "      2/1    0.000    0.000    0.072    0.072 {built-in method builtins.next}\n",
       "        1    0.000    0.000    0.000    0.000 _collections_abc.py:392(__subclasshook__)\n",
       "        2    0.000    0.000    0.000    0.000 dataloader.py:281(_auto_collation)\n",
       "        1    0.000    0.000    0.000    0.000 dataloader.py:285(_index_sampler)\n",
       "        1    0.000    0.000    0.000    0.000 tokenization_utils_base.py:989(pad_token)\n",
       "        1    0.000    0.000    0.000    0.000 datasets.py:127(__len__)\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method builtins.all}\n",
       "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%prun next(iter(data_loader_span_mlm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improved V3 Data Collator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test lab zone 👩🏻‍🔬 \n",
    "\n",
    "<br>\n",
    "\n",
    "Idea 3: k-time masking projection\n",
    "\n",
    "\n",
    "``` \n",
    "matrix = [[ . . . . . . ]\n",
    "          [ . . . . . . ]\n",
    "          [ . . . . . . ]\n",
    "          [ . . . . . . ]\n",
    "          [ . . . . . . ]]\n",
    "```\n",
    "\n",
    "Project 1-subword mask\n",
    "\n",
    "Suppose that MLM probability is 15% and the total tokens is 30, then project 1-subword mask 15% (Bernoulli distribution) or 4.5 tokens will be masked.\n",
    "\n",
    "``` \n",
    "matrix = [[ . 1 . . . . ]\n",
    "          [ . . 1 . . . ]\n",
    "          [ . . . . . 1 ]\n",
    "          [ . 1 . . . . ]\n",
    "          [ . . . 1 . . ]]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "probability_matrix:\n",
      " tensor([[0.1091, 0.1091, 0.1091, 0.1091, 0.1091, 0.1091, 0.1091, 0.1091],\n",
      "        [0.1091, 0.1091, 0.1091, 0.1091, 0.1091, 0.1091, 0.1091, 0.1091],\n",
      "        [0.1091, 0.1091, 0.1091, 0.1091, 0.1091, 0.1091, 0.1091, 0.1091],\n",
      "        [0.1091, 0.1091, 0.1091, 0.1091, 0.1091, 0.1091, 0.1091, 0.1091],\n",
      "        [0.1091, 0.1091, 0.1091, 0.1091, 0.1091, 0.1091, 0.1091, 0.1091]])\n",
      "\\masked_indices:\n",
      " tensor([[ True, False, False, False, False,  True, False, False],\n",
      "        [False, False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False],\n",
      "        [False, False, False, False,  True, False, False,  True],\n",
      "        [False, False, False, False, False, False, False, False]])\n",
      "base_indices (tensor([0, 0, 3, 3]), tensor([0, 5, 4, 7]))\n"
     ]
    }
   ],
   "source": [
    "pvals =  [0.5455, 0.2727, 0.1818]\n",
    "\n",
    "mlm_probability = .2\n",
    "probability_matrix = torch.full((5,8), mlm_probability * pvals[0])\n",
    "print('\\nprobability_matrix:\\n', probability_matrix)\n",
    "_masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "print('\\masked_indices:\\n', _masked_indices)\n",
    "\n",
    "base_indices = (_masked_indices == True).nonzero(as_tuple=True)\n",
    "print('base_indices', base_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "second_probabilty_matrix tensor([[0.0545, 0.0545, 0.0545, 0.0545, 0.0545, 0.0545, 0.0545, 0.0545],\n",
      "        [0.0545, 0.0545, 0.0545, 0.0545, 0.0545, 0.0545, 0.0545, 0.0545],\n",
      "        [0.0545, 0.0545, 0.0545, 0.0545, 0.0545, 0.0545, 0.0545, 0.0545],\n",
      "        [0.0545, 0.0545, 0.0545, 0.0545, 0.0545, 0.0545, 0.0545, 0.0545],\n",
      "        [0.0545, 0.0545, 0.0545, 0.0545, 0.0545, 0.0545, 0.0545, 0.0545]])\n",
      "second_masked_indices tensor([[False,  True, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False,  True, False],\n",
      "        [False, False, False, False, False, False, False, False]])\n",
      "second_indices (tensor([0, 3]), tensor([1, 6]))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 2-subword\n",
    "second_probabilty_matrix = torch.full((5,8), mlm_probability * pvals[1])\n",
    "print('second_probabilty_matrix', second_probabilty_matrix)\n",
    "\n",
    "second_masked_indices = torch.bernoulli(second_probabilty_matrix).bool()\n",
    "print('second_masked_indices', second_masked_indices)\n",
    "second_indices = (second_masked_indices == True).nonzero(as_tuple=True)\n",
    "\n",
    "print('second_indices', second_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "third_probabilty_matrix tensor([[0.0364, 0.0364, 0.0364, 0.0364, 0.0364, 0.0364, 0.0364, 0.0364],\n",
      "        [0.0364, 0.0364, 0.0364, 0.0364, 0.0364, 0.0364, 0.0364, 0.0364],\n",
      "        [0.0364, 0.0364, 0.0364, 0.0364, 0.0364, 0.0364, 0.0364, 0.0364],\n",
      "        [0.0364, 0.0364, 0.0364, 0.0364, 0.0364, 0.0364, 0.0364, 0.0364],\n",
      "        [0.0364, 0.0364, 0.0364, 0.0364, 0.0364, 0.0364, 0.0364, 0.0364]])\n",
      "third_masked_indices tensor([[False, False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False,  True, False, False],\n",
      "        [ True, False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False,  True, False],\n",
      "        [False, False, False, False, False, False, False, False]])\n",
      "third_indices tensor([[1, 5],\n",
      "        [2, 0],\n",
      "        [3, 6]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "third_probabilty_matrix = torch.full((5,8), mlm_probability * pvals[2])\n",
    "print('third_probabilty_matrix', third_probabilty_matrix)\n",
    "\n",
    "third_masked_indices = torch.bernoulli(third_probabilty_matrix).bool()\n",
    "print('third_masked_indices', third_masked_indices)\n",
    "third_indices = (third_masked_indices == True).nonzero(as_tuple=False)\n",
    "\n",
    "print('third_indices', third_indices)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Wrapping up in 1 for loop__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mask_tokens(inputs: torch.Tensor,\n",
    "                special_tokens_mask: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \n",
    "    labels = inputs.clone()\n",
    "    labels_to_be_mask = torch.full(inputs.shape, 0.).bool()\n",
    "    pvals =  [0.5455, 0.2727, 0.1818]\n",
    "    K = len(pvals)\n",
    "    mask_indices_by_span_len = [[] for i in range(K)]\n",
    "    \n",
    "    mlm_probability = .1\n",
    "    \n",
    "    for k in range(0, K):\n",
    "       \n",
    "            # 2-subword\n",
    "        print(f'\\n\\n{k+1}-subword masking')\n",
    "        \n",
    "        probability_matrix = torch.full(inputs.shape, mlm_probability * pvals[k] / (k+1))\n",
    "        \n",
    "        \n",
    "        print(f'\\n{k+1}-subword masking probabilty_matrix', probability_matrix)\n",
    "\n",
    "        _masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "        print(f'\\n{k+1}-subword masked_indices', _masked_indices)\n",
    "        _indices_selected = (_masked_indices == True).nonzero(as_tuple=False)\n",
    "\n",
    "        \n",
    "        print(f'\\n{k+1}-subword _indices_selected: ', _indices_selected)\n",
    "        if _indices_selected == None:\n",
    "            mask_indices_by_span_len[k] = []\n",
    "        else:\n",
    "            mask_indices_by_span_len[k] = _indices_selected\n",
    "        \n",
    "        mask_indices_by_span_len[k] = _indices_selected\n",
    "    \n",
    "    accum_indices = [[],[]]\n",
    "    max_seq_len = inputs.shape[1] - 1\n",
    "\n",
    "    for k in range(0, K):\n",
    "\n",
    "        list_of_indices = mask_indices_by_span_len[k]\n",
    "\n",
    "        if list_of_indices.shape == (0,):\n",
    "            continue\n",
    "        else:\n",
    "            for j in range(k+1):\n",
    "                max_indices = torch.full((list_of_indices.shape[0],), max_seq_len, dtype=torch.long)\n",
    "                left, right = (list_of_indices[:, 0], \\\n",
    "                               torch.min(list_of_indices[:, 1] + j, max_indices))\n",
    "\n",
    "                accum_indices[0].append(left)\n",
    "                accum_indices[1].append(right)\n",
    "    accum_indices[0] = list(filter(lambda x: x.shape != (0,), accum_indices[0]))\n",
    "    accum_indices[1] = list(filter(lambda x: x.shape != (0,), accum_indices[1]))\n",
    "    print('accum_indices', accum_indices)\n",
    "    if len(accum_indices[0]) != 0: \n",
    "        accum_indices_flatten  = (torch.cat(accum_indices[0]), torch.cat(accum_indices[1]))\n",
    "        labels_to_be_mask.index_put_(accum_indices_flatten, torch.tensor([1.]).bool())\n",
    "#         labels_to_be_mask.masked_fill_(special_tokens_mask, value=0.0).bool()\n",
    "\n",
    "    inputs[labels_to_be_mask] = 24004\n",
    "    labels[~labels_to_be_mask] = -100  # We only compute loss on masked token\n",
    "\n",
    "    return inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2,  8, 77, 62, 38,  7,  6,  4, 19, 17],\n",
       "        [98, 15, 16, 36, 94, 51, 94, 59, 45, 92],\n",
       "        [21, 49, 29, 29, 74, 17, 42, 75, 79, 97],\n",
       "        [30, 46, 91,  1, 79, 92, 28, 69, 88, 85],\n",
       "        [53, 41, 35, 86, 29, 93, 46, 15, 56, 90],\n",
       "        [74,  0,  5, 41,  5, 57, 77, 24, 74, 91],\n",
       "        [46, 41,  4, 94, 12, 43,  0, 14, 62, 63],\n",
       "        [92, 91, 17, 54, 46, 58, 95, 39, 37, 92]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = (torch.rand((8,10))*100).long() \n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1-subword masking\n",
      "\n",
      "1-subword masking probabilty_matrix tensor([[0.0545, 0.0545, 0.0545, 0.0545, 0.0545, 0.0545, 0.0545, 0.0545, 0.0545,\n",
      "         0.0545],\n",
      "        [0.0545, 0.0545, 0.0545, 0.0545, 0.0545, 0.0545, 0.0545, 0.0545, 0.0545,\n",
      "         0.0545],\n",
      "        [0.0545, 0.0545, 0.0545, 0.0545, 0.0545, 0.0545, 0.0545, 0.0545, 0.0545,\n",
      "         0.0545],\n",
      "        [0.0545, 0.0545, 0.0545, 0.0545, 0.0545, 0.0545, 0.0545, 0.0545, 0.0545,\n",
      "         0.0545],\n",
      "        [0.0545, 0.0545, 0.0545, 0.0545, 0.0545, 0.0545, 0.0545, 0.0545, 0.0545,\n",
      "         0.0545],\n",
      "        [0.0545, 0.0545, 0.0545, 0.0545, 0.0545, 0.0545, 0.0545, 0.0545, 0.0545,\n",
      "         0.0545],\n",
      "        [0.0545, 0.0545, 0.0545, 0.0545, 0.0545, 0.0545, 0.0545, 0.0545, 0.0545,\n",
      "         0.0545],\n",
      "        [0.0545, 0.0545, 0.0545, 0.0545, 0.0545, 0.0545, 0.0545, 0.0545, 0.0545,\n",
      "         0.0545]])\n",
      "\n",
      "1-subword masked_indices tensor([[False, False, False, False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False],\n",
      "        [False, False, False, False,  True, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False,  True, False],\n",
      "        [False, False, False, False, False, False, False, False, False,  True],\n",
      "        [False, False, False, False, False, False, False, False,  True, False],\n",
      "        [False, False, False, False, False, False, False, False,  True, False]])\n",
      "\n",
      "1-subword _indices_selected:  tensor([[3, 4],\n",
      "        [4, 8],\n",
      "        [5, 9],\n",
      "        [6, 8],\n",
      "        [7, 8]])\n",
      "\n",
      "\n",
      "2-subword masking\n",
      "\n",
      "2-subword masking probabilty_matrix tensor([[0.0136, 0.0136, 0.0136, 0.0136, 0.0136, 0.0136, 0.0136, 0.0136, 0.0136,\n",
      "         0.0136],\n",
      "        [0.0136, 0.0136, 0.0136, 0.0136, 0.0136, 0.0136, 0.0136, 0.0136, 0.0136,\n",
      "         0.0136],\n",
      "        [0.0136, 0.0136, 0.0136, 0.0136, 0.0136, 0.0136, 0.0136, 0.0136, 0.0136,\n",
      "         0.0136],\n",
      "        [0.0136, 0.0136, 0.0136, 0.0136, 0.0136, 0.0136, 0.0136, 0.0136, 0.0136,\n",
      "         0.0136],\n",
      "        [0.0136, 0.0136, 0.0136, 0.0136, 0.0136, 0.0136, 0.0136, 0.0136, 0.0136,\n",
      "         0.0136],\n",
      "        [0.0136, 0.0136, 0.0136, 0.0136, 0.0136, 0.0136, 0.0136, 0.0136, 0.0136,\n",
      "         0.0136],\n",
      "        [0.0136, 0.0136, 0.0136, 0.0136, 0.0136, 0.0136, 0.0136, 0.0136, 0.0136,\n",
      "         0.0136],\n",
      "        [0.0136, 0.0136, 0.0136, 0.0136, 0.0136, 0.0136, 0.0136, 0.0136, 0.0136,\n",
      "         0.0136]])\n",
      "\n",
      "2-subword masked_indices tensor([[False, False, False, False,  True, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False,  True],\n",
      "        [False, False, False, False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False]])\n",
      "\n",
      "2-subword _indices_selected:  tensor([[0, 4],\n",
      "        [3, 9]])\n",
      "\n",
      "\n",
      "3-subword masking\n",
      "\n",
      "3-subword masking probabilty_matrix tensor([[0.0061, 0.0061, 0.0061, 0.0061, 0.0061, 0.0061, 0.0061, 0.0061, 0.0061,\n",
      "         0.0061],\n",
      "        [0.0061, 0.0061, 0.0061, 0.0061, 0.0061, 0.0061, 0.0061, 0.0061, 0.0061,\n",
      "         0.0061],\n",
      "        [0.0061, 0.0061, 0.0061, 0.0061, 0.0061, 0.0061, 0.0061, 0.0061, 0.0061,\n",
      "         0.0061],\n",
      "        [0.0061, 0.0061, 0.0061, 0.0061, 0.0061, 0.0061, 0.0061, 0.0061, 0.0061,\n",
      "         0.0061],\n",
      "        [0.0061, 0.0061, 0.0061, 0.0061, 0.0061, 0.0061, 0.0061, 0.0061, 0.0061,\n",
      "         0.0061],\n",
      "        [0.0061, 0.0061, 0.0061, 0.0061, 0.0061, 0.0061, 0.0061, 0.0061, 0.0061,\n",
      "         0.0061],\n",
      "        [0.0061, 0.0061, 0.0061, 0.0061, 0.0061, 0.0061, 0.0061, 0.0061, 0.0061,\n",
      "         0.0061],\n",
      "        [0.0061, 0.0061, 0.0061, 0.0061, 0.0061, 0.0061, 0.0061, 0.0061, 0.0061,\n",
      "         0.0061]])\n",
      "\n",
      "3-subword masked_indices tensor([[False, False, False, False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False]])\n",
      "\n",
      "3-subword _indices_selected:  tensor([], size=(0, 2), dtype=torch.int64)\n",
      "accum_indices [[tensor([3, 4, 5, 6, 7]), tensor([0, 3]), tensor([0, 3])], [tensor([4, 8, 9, 8, 8]), tensor([4, 9]), tensor([5, 9])]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[    2,     8,    77,    62, 24004, 24004,     6,     4,    19,    17],\n",
       "         [   98,    15,    16,    36,    94,    51,    94,    59,    45,    92],\n",
       "         [   21,    49,    29,    29,    74,    17,    42,    75,    79,    97],\n",
       "         [   30,    46,    91,     1, 24004,    92,    28,    69,    88, 24004],\n",
       "         [24004, 24004, 24004,    86,    29,    93,    46,    15, 24004,    90],\n",
       "         [   74,     0,     5,    41,     5,    57,    77,    24,    74, 24004],\n",
       "         [   46,    41,     4,    94,    12,    43,     0,    14, 24004,    63],\n",
       "         [   92,    91,    17, 24004, 24004,    58,    95,    39, 24004,    92]]),\n",
       " tensor([[-100, -100, -100, -100,   38,    7, -100, -100, -100, -100],\n",
       "         [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
       "         [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
       "         [-100, -100, -100, -100,   79, -100, -100, -100, -100,   85],\n",
       "         [-100, -100, -100, -100, -100, -100, -100, -100,   56, -100],\n",
       "         [-100, -100, -100, -100, -100, -100, -100, -100, -100,   91],\n",
       "         [-100, -100, -100, -100, -100, -100, -100, -100,   62, -100],\n",
       "         [-100, -100, -100, -100, -100, -100, -100, -100,   37, -100]]))"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_tokens(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Garage Zone 🗺"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import List, Dict, Union, Optional, Tuple, Any\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers.data.data_collator import DataCollatorForLanguageModeling\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase\n",
    "\n",
    "SPECIAL_TOKEN_NAMES = ['bos_token', 'eos_token',\n",
    "                       'sep_token', 'cls_token', 'pad_token']\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ImprovedV3DataCollatorForSpanLevelMask(DataCollatorForLanguageModeling):\n",
    "    \"\"\"\n",
    "    Data collator used for span-level masked language modeling\n",
    "     \n",
    "    adapted from NGramMaskGenerator class\n",
    "    \n",
    "    https://github.com/microsoft/DeBERTa/blob/11fa20141d9700ba2272b38f2d5fce33d981438b/DeBERTa/apps/tasks/mlm_task.py#L36\n",
    "    and\n",
    "    https://github.com/zihangdai/xlnet/blob/0b642d14dd8aec7f1e1ecbf7d6942d5faa6be1f0/data_utils.py\n",
    "\n",
    "    \"\"\"\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    mlm: bool = True\n",
    "    mlm_probability: float = 0.15\n",
    "    max_gram: int = 3\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "\n",
    "    def __new__(cls, tokenizer, mlm, mlm_probability, pad_to_multiple_of, *args, **kwargs):\n",
    "    \n",
    "        obj = object.__new__(cls)\n",
    "        DataCollatorForLanguageModeling.__init__(obj, tokenizer=tokenizer, mlm=mlm,\n",
    "                                                 mlm_probability=mlm_probability,\n",
    "                                                 pad_to_multiple_of=pad_to_multiple_of)\n",
    "        return obj\n",
    "    \n",
    "\n",
    "    def __post_init__(self, *args, **kwargs):\n",
    "        \n",
    "        self.vocab_words = list(self.tokenizer.get_vocab().keys())\n",
    "        self.vocab_mapping = self.tokenizer.get_vocab()\n",
    "        \n",
    "        self.special_token_ids = [ self.vocab_mapping[self.tokenizer.special_tokens_map[name]] for name in  SPECIAL_TOKEN_NAMES]\n",
    "        self.ngrams = np.arange(1, self.max_gram + 1, dtype=np.int64)\n",
    "        _pvals = 1. / np.arange(1, self.max_gram + 1)\n",
    "        self.pvals = torch.Tensor(_pvals / _pvals.sum(keepdims=True))\n",
    "\n",
    "    def mask_tokens(self,inputs: torch.Tensor,\n",
    "                special_tokens_mask: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \n",
    "        labels = inputs.clone()\n",
    "        labels_to_be_mask = torch.full(inputs.shape, 0.).bool()\n",
    "\n",
    "        if special_tokens_mask is None:\n",
    "            special_tokens_mask = sum(inputs==i for i in self.special_token_ids).bool()\n",
    "        else:\n",
    "            special_tokens_mask = special_tokens_mask.bool()\n",
    "            \n",
    "        K = len(pvals)\n",
    "        mask_indices_by_span_len = [[] for i in range(K)]\n",
    "\n",
    "\n",
    "\n",
    "        for k in range(0, K):\n",
    "\n",
    "            probability_matrix = torch.full(inputs.shape, self.mlm_probability * self.pvals[k] )\n",
    "\n",
    "\n",
    "            _masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "\n",
    "            _indices_selected = (_masked_indices == True).nonzero(as_tuple=False)\n",
    "\n",
    "\n",
    "\n",
    "            if _indices_selected == None:\n",
    "                mask_indices_by_span_len[k] = []\n",
    "            else:\n",
    "                mask_indices_by_span_len[k] = _indices_selected\n",
    "\n",
    "            mask_indices_by_span_len[k] = _indices_selected\n",
    "\n",
    "        accum_indices = [[],[]]\n",
    "        max_seq_len = inputs.shape[1] - 1\n",
    "\n",
    "        for k in range(0, K):\n",
    "\n",
    "            list_of_indices = mask_indices_by_span_len[k]\n",
    "\n",
    "            if list_of_indices.shape == (0,):\n",
    "                continue\n",
    "            else:\n",
    "                for j in range(k+1):\n",
    "                    max_indices = torch.full((list_of_indices.shape[0],), max_seq_len, dtype=torch.long)\n",
    "                    left, right = (list_of_indices[:, 0], \\\n",
    "                                   torch.min(list_of_indices[:, 1] + j, max_indices))\n",
    "\n",
    "                    accum_indices[0].append(left)\n",
    "                    accum_indices[1].append(right)\n",
    "        accum_indices[0] = list(filter(lambda x: x.shape != (0,), accum_indices[0]))\n",
    "        accum_indices[1] = list(filter(lambda x: x.shape != (0,), accum_indices[1]))\n",
    "#         print('accum_indices', accum_indices)\n",
    "        if len(accum_indices[0]) != 0: \n",
    "            accum_indices_flatten  = (torch.cat(accum_indices[0]), torch.cat(accum_indices[1]))\n",
    "            labels_to_be_mask.index_put_(accum_indices_flatten, torch.tensor([1.]).bool())\n",
    "            labels_to_be_mask.masked_fill_(special_tokens_mask, value=0.0).bool()\n",
    "\n",
    "        inputs[labels_to_be_mask] = self.tokenizer.mask_token_id\n",
    "        labels[~labels_to_be_mask] = -100  # We only compute loss on masked token\n",
    "\n",
    "        return inputs, labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text 1: ▁|แผ่นดินไหว|ตาม|▁|หรือ|ทับศัพท์|ว่า|▁|อ|า|ฟ|เตอร์|ช็อก|▁|(|อังกฤษ|:|▁|af|ter|sh|ock|)|▁|เป็น|แผ่นดินไหว|ขนาดเล็ก|ที่|เกิดขึ้น|หลังจาก|แผ่นดินไหว|ขนาดใหญ่|ที่มี|ก่อนหน้า 34\n",
      "\n",
      "text 2: ▁|อ|า|ฟ|เตอร์|ยู|▁|(|After|▁|You|▁|Des|s|ert|▁|Ca|f|é|)|▁|เป็น|ร้าน|ขนมหวาน|รสชาติ|ละ|มุน|ที่|ครอง|ใจ|ลูกค้า|เป็น|อย่างดี|▁|โดยมี|เมนู|ของ|หวาน|อร่อย|ให้เลือก|หลากหลาย|เมนู|▁ 43\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "res {'input_ids': tensor([[    5, 24004,  2827,    83, 24004, 24004,  9962, 24004,     8, 24004,\n",
      "             9,   265,   712, 13225, 24004, 24004, 24004, 24004, 24004, 24004,\n",
      "         24004, 24004, 11661, 24004, 24004, 24004, 24004,   897,    12, 24004,\n",
      "           134,  2827, 24004, 24004, 24004,     6,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1],\n",
      "        [    5, 24004, 24004, 24004, 24004, 24004, 24004, 24004,    15, 23639,\n",
      "         24004, 24004,     8, 14929, 24004, 24004, 24004, 24004,   737,  2912,\n",
      "         24004,     8,    14,  1665, 20046,  6886,   674, 24004, 24004, 24004,\n",
      "           323,  3695,    14, 24004,     8, 24004, 24004, 24004, 24004, 24004,\n",
      "         24004, 24004, 24004, 24004,     6]]), 'labels': tensor([[ -100,     8,  -100,  -100,     8,    28,  -100,    38,  -100,    52,\n",
      "          -100,  -100,  -100,  -100,     8,    15,   236,    94,     8, 14051,\n",
      "          2414,  2711,  -100,    18,     8,    14,  2827,  -100,  -100,   319,\n",
      "          -100,  -100,   604,    93,   862,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100],\n",
      "        [ -100,     8,    52,     9,   265,   712,   406,     8,  -100,  -100,\n",
      "             8,  3918,  -100,  -100,    70, 13666,     8,  6101,  -100,  -100,\n",
      "            18,  -100,  -100,  -100,  -100,  -100,  -100,  5659,    12,  2226,\n",
      "          -100,  -100,  -100,  1987,  -100,   198, 14237,    17,  2858, 16595,\n",
      "         12010,   971, 14237,     8,  -100]])}\n",
      "\n",
      "inputs 1 (after):\n",
      " <s>|<mask>|แผ่นดินไหว|ตาม|<mask>|<mask>|ทับศัพท์|<mask>|▁|<mask>|า|ฟ|เตอร์|ช็อก|<mask>|<mask>|<mask>|<mask>|<mask>|<mask>|<mask>|<mask>|ock|<mask>|<mask>|<mask>|<mask>|ขนาดเล็ก|ที่|<mask>|หลังจาก|แผ่นดินไหว|<mask>|<mask>|<mask>|</s>|<pad>|<pad>|<pad>|<pad>|<pad>|<pad>|<pad>|<pad>|<pad>\n",
      "\n",
      "\n",
      "inputs 2 (after):\n",
      " <s>|<mask>|<mask>|<mask>|<mask>|<mask>|<mask>|<mask>|(|After|<mask>|<mask>|▁|Des|<mask>|<mask>|<mask>|<mask>|f|é|<mask>|▁|เป็น|ร้าน|ขนมหวาน|รสชาติ|ละ|<mask>|<mask>|<mask>|ใจ|ลูกค้า|เป็น|<mask>|▁|<mask>|<mask>|<mask>|<mask>|<mask>|<mask>|<mask>|<mask>|<mask>|</s>\n"
     ]
    }
   ],
   "source": [
    "imp_data_collator_span_mlm =  ImprovedV3DataCollatorForSpanLevelMask(tokenizer=tokenizer,\n",
    "                                              mlm=True,\n",
    "                                              mlm_probability=0.5,\n",
    "                                              max_gram=3,\n",
    "                                              pad_to_multiple_of=1)\n",
    "\n",
    "\n",
    "text1 = \"\"\"แผ่นดินไหวตาม หรือทับศัพท์ว่า อาฟเตอร์ช็อก (อังกฤษ: aftershock) เป็นแผ่นดินไหวขนาดเล็กที่เกิดขึ้นหลังจากแผ่นดินไหวขนาดใหญ่ที่มีก่อนหน้า\"\"\"\n",
    "text2 = \"\"\"อาฟเตอร์ยู (After You Dessert Café) เป็นร้านขนมหวานรสชาติละมุนที่ครองใจลูกค้าเป็นอย่างดี โดยมีเมนูของหวานอร่อยให้เลือกหลากหลายเมนู \"\"\"\n",
    "tokens = tokenizer.tokenize(text1)\n",
    "print('text 1:', '|'.join(tokens), len(tokens))\n",
    "tokens = tokenizer.tokenize(text2)\n",
    "print('\\ntext 2:','|'.join(tokens), len(tokens))\n",
    "\n",
    "\n",
    "inputs_1 = tokenizer.encode_plus(text1, return_tensors='pt')['input_ids'].squeeze(0)\n",
    "inputs_2 = tokenizer.encode_plus(text2, return_tensors='pt')['input_ids'].squeeze(0)\n",
    "\n",
    "res = imp_data_collator_span_mlm((inputs_1, inputs_2))\n",
    "print('\\n\\n')\n",
    "# print('inputs 1 (before)', inputs_1)\n",
    "# print('inputs 2 (before)', inputs_2)\n",
    "print('\\n\\n\\nres', res)\n",
    "\n",
    "print('\\ninputs 1 (after):\\n', '|'.join(tokenizer.convert_ids_to_tokens(res['input_ids'][0])))\n",
    "print('\\n\\ninputs 2 (after):\\n','|'.join(tokenizer.convert_ids_to_tokens(res['input_ids'][1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "BZ=32\n",
    "\n",
    "# data_collator_span_mlm =  DataCollatorForSpanLevelMask(tokenizer=tokenizer,\n",
    "#                                               mlm=True,\n",
    "#                                               mlm_probability=0.15,\n",
    "#                                               max_gram=3,\n",
    "#                                               keep_prob=0.0,\n",
    "#                                               mask_prob=1.0,\n",
    "\n",
    "#                                               pad_to_multiple_of=8)\n",
    "\n",
    "# data_loader_span_mlm = DataLoader(\n",
    "#             train_dataset,\n",
    "#             batch_size=BZ,\n",
    "#             sampler=train_sampler,\n",
    "#             collate_fn=data_collator_span_mlm,\n",
    "#             drop_last=False,\n",
    "#             num_workers=0,\n",
    "#             pin_memory=True,\n",
    "#         )\n",
    "\n",
    "# data_collator_subword_mlm =  DataCollatorForLanguageModeling(tokenizer=tokenizer,\n",
    "#                                               mlm=True,\n",
    "#                                               mlm_probability=0.15,\n",
    "#                                               pad_to_multiple_of=8)\n",
    "\n",
    "# data_loader_subword_mlm = DataLoader(\n",
    "#             train_dataset,\n",
    "#             batch_size=BZ,\n",
    "#             sampler=train_sampler,\n",
    "#             collate_fn=data_collator_subword_mlm,\n",
    "#             drop_last=False,\n",
    "#             num_workers=0,\n",
    "#             pin_memory=True,\n",
    "#         )\n",
    "\n",
    "imp_data_collator_span_mlm =  ImprovedV3DataCollatorForSpanLevelMask(tokenizer=tokenizer,\n",
    "                                              mlm=True,\n",
    "                                              mlm_probability=0.15,\n",
    "                                              max_gram=3,\n",
    "                                              pad_to_multiple_of=8)\n",
    "\n",
    "imp_data_loader_span_mlm = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=BZ,\n",
    "            sampler=train_sampler,\n",
    "            collate_fn=imp_data_collator_span_mlm,\n",
    "            drop_last=False,\n",
    "            num_workers=0,\n",
    "            pin_memory=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 703,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.48 ms ± 256 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "next(iter(data_loader_subword_mlm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-479-800325c8def5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'timeit'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"list(data_loader_subword_mlm)\\nprint('.', end='')\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/miniconda3/envs/saiko/lib/python3.7/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2360\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2361\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2362\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2363\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m</Users/saiko/opt/miniconda3/envs/saiko/lib/python3.7/site-packages/decorator.py:decorator-gen-60>\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/saiko/lib/python3.7/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/saiko/lib/python3.7/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1162\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1164\u001b[0;31m         \u001b[0mall_runs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1165\u001b[0m         \u001b[0mbest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_runs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnumber\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1166\u001b[0m         \u001b[0mworst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_runs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnumber\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/saiko/lib/python3.7/timeit.py\u001b[0m in \u001b[0;36mrepeat\u001b[0;34m(self, repeat, number)\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m             \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/saiko/lib/python3.7/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, number)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             \u001b[0mtiming\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mgcold\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<magic-timeit>\u001b[0m in \u001b[0;36minner\u001b[0;34m(_it, _timer)\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/saiko/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/saiko/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/saiko/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/miniconda3/envs/saiko/lib/python3.7/site-packages/transformers/data/data_collator.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, examples)\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m             batch[\"input_ids\"], batch[\"labels\"] = self.mask_tokens(\n\u001b[0;32m--> 352\u001b[0;31m                 \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspecial_tokens_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspecial_tokens_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m             )\n\u001b[1;32m    354\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/saiko/lib/python3.7/site-packages/transformers/data/data_collator.py\u001b[0m in \u001b[0;36mmask_tokens\u001b[0;34m(self, inputs, special_tokens_mask)\u001b[0m\n\u001b[1;32m    367\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[0;31m# We sample a few tokens in each sequence for MLM training (with probability `self.mlm_probability`)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 369\u001b[0;31m         \u001b[0mprobability_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlm_probability\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    370\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mspecial_tokens_mask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m             special_tokens_mask = [\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "list(data_loader_subword_mlm)\n",
    "print('.', end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "next(iter(data_loader_span_mlm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "list(data_loader_span_mlm)\n",
    "print('.', end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%prun next(iter(data_loader_span_mlm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "text/plain": [
       "         304 function calls (301 primitive calls) in 0.003 seconds\n",
       "\n",
       "   Ordered by: internal time\n",
       "\n",
       "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
       "        1    0.001    0.001    0.002    0.002 <ipython-input-118-2a5455979ea3>:50(mask_tokens)\n",
       "       11    0.001    0.000    0.001    0.000 tensor.py:25(wrapped)\n",
       "        1    0.000    0.000    0.000    0.000 data_collator.py:195(_collate_batch)\n",
       "        1    0.000    0.000    0.001    0.001 {built-in method builtins.sum}\n",
       "        3    0.000    0.000    0.000    0.000 {built-in method bernoulli}\n",
       "        7    0.000    0.000    0.000    0.000 {method 'bool' of 'torch._C._TensorBase' objects}\n",
       "        1    0.000    0.000    0.000    0.000 {method 'random_' of 'torch._C._TensorBase' objects}\n",
       "        3    0.000    0.000    0.000    0.000 {method 'nonzero' of 'torch._C._TensorBase' objects}\n",
       "       10    0.000    0.000    0.000    0.000 {built-in method full}\n",
       "        1    0.000    0.000    0.000    0.000 {method 'item' of 'torch._C._TensorBase' objects}\n",
       "        2    0.000    0.000    0.000    0.000 {built-in method cat}\n",
       "        1    0.000    0.000    0.000    0.000 {method 'new_full' of 'torch._C._TensorBase' objects}\n",
       "        1    0.000    0.000    0.000    0.000 {method 'index_put_' of 'torch._C._TensorBase' objects}\n",
       "        1    0.000    0.000    0.003    0.003 data_collator.py:339(__call__)\n",
       "        1    0.000    0.000    0.000    0.000 dataloader.py:320(__init__)\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method empty}\n",
       "        6    0.000    0.000    0.000    0.000 {built-in method min}\n",
       "        1    0.000    0.000    0.003    0.003 {built-in method builtins.exec}\n",
       "        1    0.000    0.000    0.000    0.000 {method 'clone' of 'torch._C._TensorBase' objects}\n",
       "       35    0.000    0.000    0.000    0.000 {method 'size' of 'torch._C._TensorBase' objects}\n",
       "        1    0.000    0.000    0.003    0.003 <string>:1(<module>)\n",
       "        1    0.000    0.000    0.000    0.000 {method 'masked_fill_' of 'torch._C._TensorBase' objects}\n",
       "        2    0.000    0.000    0.000    0.000 sampler.py:198(__iter__)\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method tensor}\n",
       "        2    0.000    0.000    0.000    0.000 {method 'token_to_id' of 'tokenizers.Tokenizer' objects}\n",
       "        6    0.000    0.000    0.000    0.000 <ipython-input-118-2a5455979ea3>:57(<genexpr>)\n",
       "        1    0.000    0.000    0.000    0.000 tokenization_utils_base.py:1010(mask_token)\n",
       "        1    0.000    0.000    0.000    0.000 fetch.py:44(<listcomp>)\n",
       "        1    0.000    0.000    0.000    0.000 tokenization_utils_base.py:1133(mask_token_id)\n",
       "        1    0.000    0.000    0.000    0.000 dataloader.py:375(__init__)\n",
       "       33    0.000    0.000    0.000    0.000 data_collator.py:215(<genexpr>)\n",
       "       44    0.000    0.000    0.000    0.000 {method 'append' of 'list' objects}\n",
       "    37/36    0.000    0.000    0.000    0.000 {built-in method builtins.len}\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method builtins.max}\n",
       "        1    0.000    0.000    0.003    0.003 dataloader.py:383(_next_data)\n",
       "        1    0.000    0.000    0.003    0.003 fetch.py:42(fetch)\n",
       "       32    0.000    0.000    0.000    0.000 datasets.py:130(__getitem__)\n",
       "        1    0.000    0.000    0.003    0.003 dataloader.py:344(__next__)\n",
       "        2    0.000    0.000    0.000    0.000 tokenization_utils_fast.py:242(_convert_token_to_id_with_added_voc)\n",
       "        1    0.000    0.000    0.000    0.000 sampler.py:61(__iter__)\n",
       "        1    0.000    0.000    0.000    0.000 dataloader.py:36(create_fetcher)\n",
       "        2    0.000    0.000    0.000    0.000 tokenization_utils_fast.py:220(convert_tokens_to_ids)\n",
       "      3/2    0.000    0.000    0.000    0.000 {built-in method builtins.iter}\n",
       "        6    0.000    0.000    0.000    0.000 <ipython-input-118-2a5455979ea3>:101(<lambda>)\n",
       "        6    0.000    0.000    0.000    0.000 <ipython-input-118-2a5455979ea3>:102(<lambda>)\n",
       "        1    0.000    0.000    0.000    0.000 dataloader.py:275(__iter__)\n",
       "        1    0.000    0.000    0.000    0.000 fetch.py:39(__init__)\n",
       "        1    0.000    0.000    0.000    0.000 tokenization_utils_base.py:1104(pad_token_id)\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method builtins.all}\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method builtins.hasattr}\n",
       "        4    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}\n",
       "      2/1    0.000    0.000    0.003    0.003 {built-in method builtins.next}\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method _abc._abc_instancecheck}\n",
       "        1    0.000    0.000    0.000    0.000 abc.py:137(__instancecheck__)\n",
       "        1    0.000    0.000    0.000    0.000 __init__.py:33(is_available)\n",
       "        1    0.000    0.000    0.000    0.000 tokenization_utils_base.py:989(pad_token)\n",
       "        3    0.000    0.000    0.000    0.000 data_collator.py:203(<genexpr>)\n",
       "        1    0.000    0.000    0.000    0.000 datasets.py:127(__len__)\n",
       "        1    0.000    0.000    0.000    0.000 {method 'pop' of 'dict' objects}\n",
       "        1    0.000    0.000    0.000    0.000 <ipython-input-118-2a5455979ea3>:62(<listcomp>)\n",
       "        1    0.000    0.000    0.000    0.000 dataloader.py:285(_index_sampler)\n",
       "        1    0.000    0.000    0.000    0.000 dataloader.py:338(_next_index)\n",
       "        1    0.000    0.000    0.000    0.000 fetch.py:8(__init__)\n",
       "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
       "        2    0.000    0.000    0.000    0.000 dataloader.py:281(_auto_collation)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%prun next(iter(imp_data_loader_span_mlm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %prun next(iter(data_loader_subword_mlm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.38 ms ± 111 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "next(iter(imp_data_loader_span_mlm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "list(imp_data_loader_span_mlm)\n",
    "print('.', end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res = imp_data_collator_span_mlm((inputs_1, inputs_2))\n",
    "print(res)\n",
    "\n",
    "print(sum(torch.sum(res['input_ids'].eq(1), dim=1).detach().cpu().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([14], [81])"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_mask([res])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 48])"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res['input_ids'].size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality Assurance 🥽"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_mask(results):\n",
    "    mask_counts = []\n",
    "    token_counts = []\n",
    "    for item in results:\n",
    "#         print(item['labels'])\n",
    "        mask_count = sum(torch.sum(~(item['labels'].eq(-100)), dim=1).detach().cpu().numpy())\n",
    "        special_tokens_count = sum(torch.sum(( item['input_ids'].eq(1) | item['input_ids'].eq(5)  |item['input_ids'].eq(6) ), dim=1).detach().cpu().numpy())\n",
    "\n",
    "        _token_count = item['input_ids'].shape[0] * item['input_ids'].shape[1]\n",
    "#         print('_token_count', _token_count)\n",
    "        token_count = _token_count - special_tokens_count\n",
    "#         print('token_count', token_count)\n",
    "        token_counts.append(token_count)\n",
    "        mask_counts.append(mask_count)\n",
    "    return mask_counts, token_counts\n",
    "\n",
    "\n",
    "def run_exp_masking_percentage(data_loader):\n",
    "    \n",
    "    result = list(data_loader)\n",
    "    \n",
    "    mask_counts, token_counts = count_mask(result)\n",
    "#     print('mask_counts', mask_counts)\n",
    "#     print('token_counts', token_counts)\n",
    "    total_mask_tokens = sum(mask_counts)\n",
    "    total_tokens = sum(token_counts)\n",
    "    percentage = total_mask_tokens / total_tokens * 100\n",
    "    print(f'total_mask_tokens: {total_mask_tokens}')\n",
    "    print(f'total_tokens: {total_tokens}')\n",
    "    print(f'masking percentage: {percentage:.3f}')\n",
    "    return percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "BZ=32\n",
    "imp_data_collator_span_mlm =  ImprovedV3DataCollatorForSpanLevelMask(tokenizer=tokenizer,\n",
    "                                              mlm=True,\n",
    "                                              mlm_probability=0.3,\n",
    "                                              max_gram=3,\n",
    "                                              pad_to_multiple_of=8)\n",
    "\n",
    "imp_data_loader_span_mlm = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=BZ,\n",
    "            sampler=train_sampler,\n",
    "            collate_fn=imp_data_collator_span_mlm,\n",
    "            drop_last=False,\n",
    "            num_workers=0,\n",
    "            pin_memory=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "exp 1\n",
      "total_mask_tokens: 652566\n",
      "total_tokens: 1616963\n",
      "masking percentage: 40.358\n",
      "\n",
      "\n",
      "exp 2\n",
      "total_mask_tokens: 654079\n",
      "total_tokens: 1616963\n",
      "masking percentage: 40.451\n",
      "\n",
      "\n",
      "exp 3\n",
      "total_mask_tokens: 651763\n",
      "total_tokens: 1616963\n",
      "masking percentage: 40.308\n",
      "\n",
      "\n",
      "exp 4\n",
      "total_mask_tokens: 651896\n",
      "total_tokens: 1616963\n",
      "masking percentage: 40.316\n",
      "\n",
      "\n",
      "exp 5\n",
      "total_mask_tokens: 651896\n",
      "total_tokens: 1616963\n",
      "masking percentage: 40.316\n",
      "\n",
      "\n",
      "exp 6\n",
      "total_mask_tokens: 651645\n",
      "total_tokens: 1616963\n",
      "masking percentage: 40.301\n",
      "\n",
      "\n",
      "exp 7\n",
      "total_mask_tokens: 653940\n",
      "total_tokens: 1616963\n",
      "masking percentage: 40.442\n",
      "\n",
      "\n",
      "exp 8\n",
      "total_mask_tokens: 652761\n",
      "total_tokens: 1616963\n",
      "masking percentage: 40.370\n",
      "\n",
      "\n",
      "exp 9\n",
      "total_mask_tokens: 653821\n",
      "total_tokens: 1616963\n",
      "masking percentage: 40.435\n",
      "\n",
      "\n",
      "exp 10\n",
      "total_mask_tokens: 650904\n",
      "total_tokens: 1616963\n",
      "masking percentage: 40.255\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    10.000000\n",
       "mean     40.355104\n",
       "std       0.068148\n",
       "min      40.254724\n",
       "25%      40.309905\n",
       "50%      40.336792\n",
       "75%      40.418736\n",
       "max      40.451080\n",
       "dtype: float64"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percentages = []\n",
    "for i in range(10):\n",
    "    print(f'\\n\\nexp {i+1}')\n",
    "    percentages.append(run_exp_masking_percentage(data_loader=imp_data_loader_span_mlm))\n",
    "    \n",
    "percentages_df = pd.Series(percentages)\n",
    "percentages_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset_large' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-127-cc84d419fa40>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m imp_data_loader_span_mlm_large = DataLoader(\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0mtrain_dataset_large\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBZ\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0msampler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_sampler\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dataset_large' is not defined"
     ]
    }
   ],
   "source": [
    "BZ = 32\n",
    "imp_data_collator_span_mlm =  ImprovedV3DataCollatorForSpanLevelMask(tokenizer=tokenizer,\n",
    "                                              mlm=True,\n",
    "                                              mlm_probability=.2,\n",
    "                                              max_gram=5,\n",
    "\n",
    "                                              pad_to_multiple_of=8)\n",
    "\n",
    "imp_data_loader_span_mlm_large = DataLoader(\n",
    "            train_dataset_large,\n",
    "            batch_size=BZ,\n",
    "            sampler=train_sampler,\n",
    "            collate_fn=imp_data_collator_span_mlm,\n",
    "            drop_last=False,\n",
    "            num_workers=0,\n",
    "            pin_memory=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "exp 1\n",
      "total_mask_tokens: 217076\n",
      "total_tokens: 983751\n",
      "masking percentage: 22.066\n",
      "\n",
      "\n",
      "exp 2\n",
      "total_mask_tokens: 217684\n",
      "total_tokens: 983751\n",
      "masking percentage: 22.128\n",
      "\n",
      "\n",
      "exp 3\n",
      "total_mask_tokens: 217728\n",
      "total_tokens: 983751\n",
      "masking percentage: 22.132\n",
      "\n",
      "\n",
      "exp 4\n",
      "total_mask_tokens: 217454\n",
      "total_tokens: 983751\n",
      "masking percentage: 22.105\n",
      "\n",
      "\n",
      "exp 5\n",
      "total_mask_tokens: 216778\n",
      "total_tokens: 983751\n",
      "masking percentage: 22.036\n",
      "\n",
      "\n",
      "exp 6\n",
      "total_mask_tokens: 217589\n",
      "total_tokens: 983751\n",
      "masking percentage: 22.118\n",
      "\n",
      "\n",
      "exp 7\n",
      "total_mask_tokens: 217448\n",
      "total_tokens: 983751\n",
      "masking percentage: 22.104\n",
      "\n",
      "\n",
      "exp 8\n",
      "total_mask_tokens: 217073\n",
      "total_tokens: 983751\n",
      "masking percentage: 22.066\n",
      "\n",
      "\n",
      "exp 9\n",
      "total_mask_tokens: 216243\n",
      "total_tokens: 983751\n",
      "masking percentage: 21.981\n",
      "\n",
      "\n",
      "exp 10\n",
      "total_mask_tokens: 215945\n",
      "total_tokens: 983751\n",
      "masking percentage: 21.951\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    10.000000\n",
       "mean     22.068776\n",
       "std       0.062427\n",
       "min      21.951185\n",
       "25%      22.043358\n",
       "50%      22.085060\n",
       "75%      22.114870\n",
       "max      22.132430\n",
       "dtype: float64"
      ]
     },
     "execution_count": 1261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percentages = []\n",
    "for i in range(10):\n",
    "    print(f'\\n\\nexp {i+1}')\n",
    "    percentages.append(run_exp_masking_percentage(data_loader=imp_data_loader_span_mlm_large))\n",
    "    \n",
    "percentages_df = pd.Series(percentages)\n",
    "percentages_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc-autonumbering": false,
  "toc-showcode": true,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
