{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler\n",
    "%load_ext memory_profiler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('airesearchth/wangchanberta-base-wiki-20210520-spm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPECIAL_TOKEN_NAMES = ['bos_token', 'eos_token', 'sep_token', 'pad_token', 'cls_token', 'mask_token']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pvals [1.         0.5        0.33333333]\n",
      "pvals norm [0.54545455 0.27272727 0.18181818]\n",
      "pvals sum 1.0000000000000002\n"
     ]
    }
   ],
   "source": [
    "pvals = 1. / np.arange(1, 3 + 1)\n",
    "print('pvals', pvals)\n",
    "pvals /= pvals.sum(keepdims=True)\n",
    "print('pvals norm', pvals)\n",
    "\n",
    "print('pvals sum', sum(pvals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p=0.2\n",
    "# k=3\n",
    "\n",
    "# [ geometric_dist(k=k, p=p)for k in range(1,3+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PreTrainedTokenizer(name_or_path='airesearchth/wangchanberta-base-wiki-20210520-spm', vocab_size=24005, model_max_len=1000000000000000019884624838656, is_fast=False, padding_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>', 'additional_special_tokens': ['<s>NOTUSED', '</s>NOTUSED', '▁']}),\n",
       " '<mask>')"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer, tokenizer.mask_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    5,     6,     3,     6,     1,     5, 24004,     0,     2,     8,\n",
      "            58]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0]]"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vals = torch.LongTensor([tokenizer.convert_tokens_to_ids(['<s>', '</s>', '<unk>', '</s>', '<pad>', '<s>', '<mask>', '<s>NOTUSED', '</s>NOTUSED', '▁', 'ก'])])\n",
    "print(vals)\n",
    "\n",
    "[ tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in vals]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bos_token': '<s>',\n",
       " 'eos_token': '</s>',\n",
       " 'unk_token': '<unk>',\n",
       " 'sep_token': '</s>',\n",
       " 'pad_token': '<pad>',\n",
       " 'cls_token': '<s>',\n",
       " 'mask_token': '<mask>',\n",
       " 'additional_special_tokens': \"['<s>NOTUSED', '</s>NOTUSED', '▁']\"}"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.special_tokens_map\n",
    "# '<s>', '</s>', '<unk>', '</s>', '<pad>', '<s>', '<mask>', '<s>NOTUSED', '</s>NOTUSED', '▁'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " self.max_preds_per_seq 80\n"
     ]
    }
   ],
   "source": [
    "mask_generator = NGramMaskGenerator(tokenizer, max_gram=3, mlm_prob=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ภาษา', 'อินโด', '-', 'ยูโรเปียน', 'ดั้งเดิม', '▁', 'คือ', 'ภาษา', 'สืบ', 'สร้าง', 'ซึ่งเป็น', 'ภาษา', 'จากการ', 'สมมุติฐาน', 'ที่', 'คาดว่า', 'บรรพบุรุษ', 'ของ', 'อินโด', '-', 'ยูโรเปียน', 'เคย', 'พูดภาษา', 'เดียวกัน', '▁', 'ภาษา', 'อินโด', '-', 'ยูโรเปียน', 'ดั้งเดิม', 'คือ', 'ภาษา', 'ดั้งเดิม', 'ที่เป็น', 'ที่สนใจ', 'มากที่สุด', '▁', 'รวมถึง', 'เข้าใจ', 'มากที่สุด', 'อีกด้วย', '▁', 'งาน', 'ส่วนใหญ่', 'ของนัก', 'ภาษาศาสตร์', 'ในช่วง', 'ศตวรรษที่', '▁', '19', '▁', 'มักจะเป็น', 'เรื่อง', 'บูรณะ', 'ภาษานี้', '▁', 'และภาษา', 'ลูกหลาน', 'เช่น', '▁', 'ภาษา', 'เจอร์แมนิก', 'ดั้งเดิม'] 63\n"
     ]
    }
   ],
   "source": [
    "# text = \"We present SpanBERT, a pre-training method that is designed to better represent and predict spans of text.\"\n",
    "text = \"\"\"ภาษาอินโด-ยูโรเปียนดั้งเดิม คือภาษาสืบสร้างซึ่งเป็นภาษาจากการสมมุติฐานที่คาดว่าบรรพบุรุษของอินโด-ยูโรเปียนเคยพูดภาษาเดียวกัน ภาษาอินโด-ยูโรเปียนดั้งเดิมคือภาษาดั้งเดิมที่เป็นที่สนใจมากที่สุด \n",
    "รวมถึงเข้าใจมากที่สุดอีกด้วย งานส่วนใหญ่ของนักภาษาศาสตร์ในช่วงศตวรรษที่ 19 มักจะเป็นเรื่องบูรณะภาษานี้ และภาษาลูกหลานเช่น ภาษาเจอร์แมนิกดั้งเดิม\"\"\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(tokens, len(tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ภาษา', 'อินโด', '-', 'ยูโรเปียน', 'ดั้งเดิม', '<mask>', 'คือ', 'ภาษา', '<mask>', 'สร้าง', 'ซึ่งเป็น', 'ภาษา', '<mask>', 'สมมุติฐาน', 'ที่', 'คาดว่า', 'บรรพบุรุษ', 'ของ', 'อินโด', '<mask>', 'ยูโรเปียน', 'เคย', 'พูดภาษา', 'เดียวกัน', '▁', 'ภาษา', 'อินโด', '<mask>', 'ยูโรเปียน', 'ดั้งเดิม', 'คือ', 'ภาษา', 'ดั้งเดิม', '<mask>', 'ที่สนใจ', 'มากที่สุด', '▁', 'รวมถึง', 'เข้าใจ', 'มากที่สุด', '<mask>', '▁', 'งาน', 'ส่วนใหญ่', '<mask>', 'ภาษาศาสตร์', 'ในช่วง', 'ศตวรรษที่', '▁', '19', '▁', '<mask>', 'เรื่อง', 'บูรณะ', 'ภาษานี้', '▁', 'และภาษา', 'ลูกหลาน', 'เช่น', '▁', 'ภาษา', 'เจอร์แมนิก', 'ดั้งเดิม']\n"
     ]
    }
   ],
   "source": [
    "tokens, labels = mask_generator.mask_tokens(tokens=tokens, rng=rng)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>NOTUSED', '<s>NOTUSED', '<s>NOTUSED', '<s>NOTUSED', '<s>NOTUSED', '▁', '<s>NOTUSED', '<s>NOTUSED', 'สืบ', '<s>NOTUSED', '<s>NOTUSED', '<s>NOTUSED', 'จากการ', '<s>NOTUSED', '<s>NOTUSED', '<s>NOTUSED', '<s>NOTUSED', '<s>NOTUSED', '<s>NOTUSED', '-', '<s>NOTUSED', '<s>NOTUSED', '<s>NOTUSED', '<s>NOTUSED', '<s>NOTUSED', '<s>NOTUSED', '<s>NOTUSED', '-', '<s>NOTUSED', '<s>NOTUSED', '<s>NOTUSED', '<s>NOTUSED', '<s>NOTUSED', 'ที่เป็น', '<s>NOTUSED', '<s>NOTUSED', '<s>NOTUSED', '<s>NOTUSED', '<s>NOTUSED', '<s>NOTUSED', 'อีกด้วย', '<s>NOTUSED', '<s>NOTUSED', '<s>NOTUSED', 'ของนัก', '<s>NOTUSED', '<s>NOTUSED', '<s>NOTUSED', '<s>NOTUSED', '<s>NOTUSED', '<s>NOTUSED', 'มักจะเป็น', '<s>NOTUSED', '<s>NOTUSED', '<s>NOTUSED', '<s>NOTUSED', '<s>NOTUSED', '<s>NOTUSED', '<s>NOTUSED', '<s>NOTUSED', '<s>NOTUSED', '<s>NOTUSED', '<s>NOTUSED']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_ids_to_tokens(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
